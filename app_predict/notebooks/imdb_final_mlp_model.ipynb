{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The best MLP model for imdb Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is the final retraining best MLP model, which has been selected in `imdb_mlp_select_structure.ipynb`.\n",
    "For the retrain let's train model with the training and the validation subsets. The model will be trained in 25 epochs. \n",
    "This number of epochs meets the epoch number with the minimum value of the loss function on the validation subset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From the previous step the model has been selected:**  \n",
    "* `learning_rate=1e-4`,\n",
    "* `epochs=1000`,\n",
    "* `batch_size=128`,\n",
    "* `layers=3`,\n",
    "* `units=32`,\n",
    "* `dropout_rate=0.6`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Import libraries,  import custom scripts and define constants  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import re\n",
    "import pickle\n",
    "import json\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,inspect\n",
    "currentdir=os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir=os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir)\n",
    "from src import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definition constants\n",
    "RANDOM_STATE = 11\n",
    "TARGET_METRIC = 'f1'\n",
    "TEST_SIZE = 0.15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  One of the other reviewers has mentioned that ...          1\n",
       "1  A wonderful little production. <br /><br />The...          1\n",
       "2  I thought this was a wonderful way to spend ti...          1\n",
       "3  Basically there's a family where a little boy ...          0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...          1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import & display data\n",
    "data = pd.read_csv('../../data/IMDB_Dataset.csv')\n",
    "data['sentiment'] = data['sentiment'].replace({'positive' : 1, 'negative' : 0})\n",
    "data = data.drop_duplicates()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Split data for training, testing and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.review\n",
    "y = data.sentiment\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y,\n",
    "                                                    test_size=TEST_SIZE, \n",
    "                                                    random_state=RANDOM_STATE, \n",
    "                                                    stratify = y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For pre-processing and vectorization, let's use the approach from the previous stage, which received the best performance.\n",
    "\n",
    "For pre-processing:\n",
    "* the removal of html-tags,\n",
    "* the separation of numbers and letters.  \n",
    "\n",
    "For vectorization we use  `TfidfVectorizer(ngram_range=(1,2))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=50000, min_df=1,\n",
       "        ngram_range=(1, 2), norm='l2',\n",
       "        preprocessor=<function prep...f=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, use_idf=True, vocabulary=None))])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mlp_final_preproc = Pipeline([\n",
    "    ('vect', TfidfVectorizer(ngram_range=(1,2), \n",
    "                             preprocessor=preprocessing.preprocessing_text, \n",
    "                             max_features=MAX_FEATURES)),\n",
    "])\n",
    "\n",
    "mlp_final_preproc.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the model\n",
    "Define the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for f1 metric\n",
    "def get_f1(y_true, y_pred): \n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model(layers=3, units=32, dropout_rate=0.6, input_shape=(MAX_FEATURES, ), learning_rate=1e-4 ):\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(rate=dropout_rate, input_shape=input_shape))\n",
    "    for _ in range(layers-1):\n",
    "        model.add(Dense(units=units, activation='relu'))\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    loss = 'binary_crossentropy'\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[get_f1])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout (Dropout)            (None, 50000)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                1600032   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,601,121\n",
      "Trainable params: 1,601,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "final_mlp_model = mlp_model()\n",
    "final_mlp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the final MLP model and save it to the destination folder: *`../service/model/mlp/`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '../service/model/mlp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      " - 45s - loss: 0.6877 - get_f1: 0.5027\n",
      "Epoch 2/25\n",
      " - 46s - loss: 0.6456 - get_f1: 0.7147\n",
      "Epoch 3/25\n",
      " - 43s - loss: 0.5587 - get_f1: 0.7898\n",
      "Epoch 4/25\n",
      " - 44s - loss: 0.4682 - get_f1: 0.8313\n",
      "Epoch 5/25\n",
      " - 60s - loss: 0.4027 - get_f1: 0.8581\n",
      "Epoch 6/25\n",
      " - 56s - loss: 0.3568 - get_f1: 0.8716\n",
      "Epoch 7/25\n",
      " - 47s - loss: 0.3218 - get_f1: 0.8838\n",
      "Epoch 8/25\n",
      " - 48s - loss: 0.2961 - get_f1: 0.8941\n",
      "Epoch 9/25\n",
      " - 45s - loss: 0.2805 - get_f1: 0.8998\n",
      "Epoch 10/25\n",
      " - 44s - loss: 0.2664 - get_f1: 0.9060\n",
      "Epoch 11/25\n",
      " - 45s - loss: 0.2539 - get_f1: 0.9100\n",
      "Epoch 12/25\n",
      " - 47s - loss: 0.2455 - get_f1: 0.9140\n",
      "Epoch 13/25\n",
      " - 50s - loss: 0.2370 - get_f1: 0.9164\n",
      "Epoch 14/25\n",
      " - 48s - loss: 0.2315 - get_f1: 0.9184\n",
      "Epoch 15/25\n",
      " - 46s - loss: 0.2221 - get_f1: 0.9216\n",
      "Epoch 16/25\n",
      " - 47s - loss: 0.2195 - get_f1: 0.9224\n",
      "Epoch 17/25\n",
      " - 46s - loss: 0.2091 - get_f1: 0.9257\n",
      "Epoch 18/25\n",
      " - 49s - loss: 0.2039 - get_f1: 0.9270\n",
      "Epoch 19/25\n",
      " - 48s - loss: 0.1998 - get_f1: 0.9285\n",
      "Epoch 20/25\n",
      " - 50s - loss: 0.1987 - get_f1: 0.9283\n",
      "Epoch 21/25\n",
      " - 49s - loss: 0.1915 - get_f1: 0.9319\n",
      "Epoch 22/25\n",
      " - 51s - loss: 0.1889 - get_f1: 0.9315\n",
      "Epoch 23/25\n",
      " - 45s - loss: 0.1894 - get_f1: 0.9315\n",
      "Epoch 24/25\n",
      " - 50s - loss: 0.1837 - get_f1: 0.9346\n",
      "Epoch 25/25\n",
      "\n",
      "Epoch 00025: saving model to ../service/model/mlp//mlp_final.hdf5\n",
      " - 48s - loss: 0.1844 - get_f1: 0.9344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x220dc31a308>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 25\n",
    "checkpoint_path = folder_path + '/mlp_final.hdf5'\n",
    "log_dir = 'logs/mlp_final'\n",
    "\n",
    "cp_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_path, \n",
    "    verbose=2, \n",
    "    save_weights_only=False,\n",
    "    period=25)\n",
    "\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=0)\n",
    "\n",
    "final_mlp_model.fit(mlp_final_preproc.transform(X_train),\n",
    "                    y_train,\n",
    "                    epochs=epochs,\n",
    "                    verbose=2, \n",
    "                    callbacks=[cp_callback, tensorboard_callback],\n",
    "                    batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model with test subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: 0.2121 - get_f1: 0.9141\n",
      "F1 score with the best MLP model for test dataset: 91.41%\n"
     ]
    }
   ],
   "source": [
    "loss, f1_score = final_mlp_model.evaluate(mlp_final_preproc.transform(X_test),  y_test, verbose=2)\n",
    "print(\"F1 score with the best MLP model for test dataset: {:5.2f}%\".format(100*f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the preprocessing and the additional information about model to disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the preprocessing to disk with pickle\n",
    "mlp_preproc_file_name = folder_path +  \"mlp_preproc.pkl\"  \n",
    "\n",
    "with open(mlp_preproc_file_name, 'wb') as file:  \n",
    "    pickle.dump(mlp_final_preproc, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the metadata to model \n",
    "\n",
    "metadata_to_model = {\n",
    "    'vectorizer' : str(mlp_final_pipeline.steps[0][1]),\n",
    "    'model_type': 'MLP NN model',\n",
    "    'author': 'Tatsiana Drabysheuskaya',\n",
    "    'data' : str(date.today()),\n",
    "    'trainig_data' : 'https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews',\n",
    "    'metrics_test_data_set': {\n",
    "        'f1_score': \"{:5.2f}%\".format(100*f1_score)\n",
    "    }\n",
    "    }\n",
    "\n",
    "\n",
    "metadata_file_name = folder_path + \"mlp_model.json\"  \n",
    "    \n",
    "with open(metadata_file_name, 'w') as file:\n",
    "    json_string = json.dumps(metadata_to_model, default=lambda o: o.__dict__, sort_keys=True, indent=2)\n",
    "    file.write(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a few test samples \n",
    "test_samples_file_name = folder_path + \"mlp_modelTest_samples.csv\"  \n",
    "\n",
    "test_df = pd.DataFrame({'review': X_test, 'sentiment': y_test})\n",
    "test_df.sample(n=8, random_state=RANDOM_STATE).to_csv(test_samples_file_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '../service/model/mlp/'\n",
    "mlp_preproc_file_name = folder_path +  \"mlp_preproc.pkl\" \n",
    "checkpoint_path = folder_path + '/mlp_final.hdf5'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and check the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl_model_loaded = load_model(checkpoint_path, custom_objects={'get_f1' : get_f1} )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(mlp_preproc_file_name, 'rb') as file:\n",
    "    mlp_preproc_loaded = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: 0.2121 - get_f1: 0.9141\n"
     ]
    }
   ],
   "source": [
    "loss, f1_score = mpl_model_loaded.evaluate(mlp_preproc_loaded.transform(X_test),  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
