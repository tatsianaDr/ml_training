{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Problem Undestanding\n",
    "Here we will look at a Data Science challenge within the IMDB space. For our model fitting choose the f1-score metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries & data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters for display dataframe\n",
    "pd.set_option('precision', 4)\n",
    "pd.set_option('max_colwidth', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definition constants\n",
    "RANDOM_STATE = 11\n",
    "MAX_ITER=4000\n",
    "NUMBER_K_FOLD = 5\n",
    "TARGET_METRIC = 'f1'\n",
    "TEST_SIZE = 0.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Preprocessing</th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>f1 cv</th>\n",
       "      <th>f1 test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Model, Preprocessing, Vectorizer, f1 cv, f1 test]\n",
       "Index: []"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Results of exploring for model we will provided in table model_search_result\n",
    "models_research_df = pd.DataFrame(columns=['Model',\n",
    "                                            'Preprocessing',\n",
    "                                            'Vectorizer',\n",
    "                                            'f1 cv',\n",
    "                                            'f1 test', ], )\n",
    "models_research_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add data to result research dataframe\n",
    "def add_data_df(table_df_for_add, model_research: tuple):\n",
    "    if any([list(row.values ) == list(model_research) for _, row in table_df_for_add.iterrows()]):\n",
    "        return\n",
    "    table_df_for_add.loc[len(table_df_for_add)] = model_research\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_f1_curve(title: str, lines: tuple):\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.title(title)\n",
    "    for line in lines:\n",
    "        plt.plot(range(0, len(line)),line, label=line.name)\n",
    "    plt.xlabel('Index the model from the result  table')\n",
    "    plt.ylabel('f1')\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import & display data\n",
    "data = pd.read_csv('../../data/IMDB_Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data\n",
    "Our training set has 50K movie reviews for natural language processing.  This is a dataser for binary sentiment classification. \n",
    "For more dataset information, please go through the following link,\n",
    "https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.&lt;br /&gt;&lt;br /&gt;The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO....</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. &lt;br /&gt;&lt;br /&gt;The actors are extremely well chosen- Michael Sheen not only \"has got all the pola...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet &amp; his parents are fighting all the time.&lt;br /&gt;&lt;br /&gt;This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.&lt;br /&gt;&lt;br /&gt;OK, first of all when you're going t...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what money, power and success do to people in the different situations we encounter. &lt;br /&gt;&lt;br /&gt;This being a...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                        review  \\\n",
       "0  One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO....   \n",
       "1  A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"has got all the pola...   \n",
       "2  I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may...   \n",
       "3  Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.<br /><br />OK, first of all when you're going t...   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what money, power and success do to people in the different situations we encounter. <br /><br />This being a...   \n",
       "\n",
       "  sentiment  \n",
       "0  positive  \n",
       "1  positive  \n",
       "2  positive  \n",
       "3  negative  \n",
       "4  positive  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review       50000\n",
       "sentiment    50000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>49582</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Loved today's show!!! It was a variety and not solely cooking (which would have been great too). Very stimulating and captivating, always keeping the viewer peeking around the corner to see what was coming up next. She is as down to earth and as personable as you get, like one of us which made t...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                             review  \\\n",
       "count                                                                                                                                                                                                                                                                                                         50000   \n",
       "unique                                                                                                                                                                                                                                                                                                        49582   \n",
       "top     Loved today's show!!! It was a variety and not solely cooking (which would have been great too). Very stimulating and captivating, always keeping the viewer peeking around the corner to see what was coming up next. She is as down to earth and as personable as you get, like one of us which made t...   \n",
       "freq                                                                                                                                                                                                                                                                                                              5   \n",
       "\n",
       "       sentiment  \n",
       "count      50000  \n",
       "unique         2  \n",
       "top     negative  \n",
       "freq       25000  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataset contains invalid non-unique values. In the next step research we should drop all data repetitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3537</th>\n",
       "      <td>Quite what the producers of this appalling adaptation were trying to do is impossible to fathom.&lt;br /&gt;&lt;br /&gt;A group of top quality actors, in the main well cast (with a couple of notable exceptions), who give pretty good performances. Penelope Keith is perfect as Aunt Louise and equally good is ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3769</th>\n",
       "      <td>My favourite police series of all time turns to a TV-film. Does it work? Yes. Gee runs for mayor and gets shot. The Homicide \"hall of fame\" turns up. Pembleton and nearly all of the cops who ever played in this series. A lot of flashbacks helps you who hasn´t seen the TV-series but it amuses the...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4391</th>\n",
       "      <td>Beautiful film, pure Cassavetes style. Gena Rowland gives a stunning performance of a declining actress, dealing with success, aging, loneliness...and alcoholism. She tries to escape her own subconscious ghosts, embodied by the death spectre of a young girl. Acceptance of oneself, of human condi...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6352</th>\n",
       "      <td>If you liked the Grinch movie... go watch that again, because this was no where near as good a Seussian movie translation. Mike Myers' Cat is probably the most annoying character to \"grace\" the screen in recent times. His voice/accent is terrible and he laughs at his own jokes with an awful weas...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6479</th>\n",
       "      <td>I want very much to believe that the above quote (specifically, the English subtitle translation), which was actually written, not spoken, in a rejection letter a publisher sends to the protagonist, was meant to be self-referential in a tongue-in-cheek manner. But if so, director Leos Carax appa...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                           review  \\\n",
       "3537  Quite what the producers of this appalling adaptation were trying to do is impossible to fathom.<br /><br />A group of top quality actors, in the main well cast (with a couple of notable exceptions), who give pretty good performances. Penelope Keith is perfect as Aunt Louise and equally good is ...   \n",
       "3769  My favourite police series of all time turns to a TV-film. Does it work? Yes. Gee runs for mayor and gets shot. The Homicide \"hall of fame\" turns up. Pembleton and nearly all of the cops who ever played in this series. A lot of flashbacks helps you who hasn´t seen the TV-series but it amuses the...   \n",
       "4391  Beautiful film, pure Cassavetes style. Gena Rowland gives a stunning performance of a declining actress, dealing with success, aging, loneliness...and alcoholism. She tries to escape her own subconscious ghosts, embodied by the death spectre of a young girl. Acceptance of oneself, of human condi...   \n",
       "6352  If you liked the Grinch movie... go watch that again, because this was no where near as good a Seussian movie translation. Mike Myers' Cat is probably the most annoying character to \"grace\" the screen in recent times. His voice/accent is terrible and he laughs at his own jokes with an awful weas...   \n",
       "6479  I want very much to believe that the above quote (specifically, the English subtitle translation), which was actually written, not spoken, in a rejection letter a publisher sends to the protagonist, was meant to be self-referential in a tongue-in-cheek manner. But if so, director Leos Carax appa...   \n",
       "\n",
       "     sentiment  \n",
       "3537  negative  \n",
       "3769  positive  \n",
       "4391  positive  \n",
       "6352  negative  \n",
       "6479  negative  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.duplicated()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### replace the categoric values from 'sentiment' to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sentiment'] = data['sentiment'].replace({'positive' : 1, 'negative' : 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.&lt;br /&gt;&lt;br /&gt;The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. &lt;br /&gt;&lt;br /&gt;The actors are extremely well chosen- Michael Sheen not only \"has got all the pola...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet &amp; his parents are fighting all the time.&lt;br /&gt;&lt;br /&gt;This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.&lt;br /&gt;&lt;br /&gt;OK, first of all when you're going t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what money, power and success do to people in the different situations we encounter. &lt;br /&gt;&lt;br /&gt;This being a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                        review  \\\n",
       "0  One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO....   \n",
       "1  A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"has got all the pola...   \n",
       "2  I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may...   \n",
       "3  Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.<br /><br />OK, first of all when you're going t...   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what money, power and success do to people in the different situations we encounter. <br /><br />This being a...   \n",
       "\n",
       "   sentiment  \n",
       "0          1  \n",
       "1          1  \n",
       "2          1  \n",
       "3          0  \n",
       "4          1  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Explore models with preprocessing and vectoriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start work with Logistic Regression for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams_C = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "\n",
    "#function for find out the samples with particular text\n",
    "def find_out_samples_with_text(data_for_explore, template):\n",
    "    return [review for review in data_for_explore['review'] if  template in review.lower()]\n",
    "\n",
    "#function for exploring data and fit model\n",
    "def grid_search_cv(train: tuple, parameters: list, vectorizer, model_fn):\n",
    "    kf = KFold(n_splits=NUMBER_K_FOLD)\n",
    "    \n",
    "    gs_metrics = {}\n",
    "    for parameter in parameters:\n",
    "        cv_metrics = []\n",
    "        \n",
    "        folds = kf.split(train[0])\n",
    "        for train_index, val_index in folds:\n",
    "            fold_X_train, fold_X_val = train[0][train_index], train[0][val_index]\n",
    "            fold_y_train, fold_y_val = train[1][train_index], train[1][val_index]\n",
    "            \n",
    "            vectorizer.fit(fold_X_train)\n",
    "            fold_X_train_features = vectorizer.transform(fold_X_train)\n",
    "            \n",
    "            fold_X_val_features = vectorizer.transform(fold_X_val)\n",
    "            \n",
    "            model = model_fn(C=parameter, random_state=RANDOM_STATE, max_iter=MAX_ITER)\n",
    "            model.fit(fold_X_train_features, fold_y_train)\n",
    "\n",
    "            metric = f1_score(model.predict(fold_X_val_features), fold_y_val)\n",
    "            cv_metrics.append(metric)\n",
    "            \n",
    "            cv_score = np.mean(cv_metrics)\n",
    "            \n",
    "            gs_metrics[parameter] = cv_score\n",
    "    return sorted(gs_metrics.items(), key=lambda x: x[1], reverse=True)[0]\n",
    "\n",
    "def model_exploring(data_set: tuple, preprocessing_fn, vectorizer, model_fn, parameters: list):\n",
    "    data_set = preprocessing_fn(data_set)\n",
    "    X = data_set[0]\n",
    "    y = data_set[1]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                        y,\n",
    "                                                        test_size=TEST_SIZE, \n",
    "                                                        random_state=RANDOM_STATE, \n",
    "                                                        stratify = y)\n",
    "    X_train_features = vectorizer.fit(X_train)\n",
    "    X_train_features = vectorizer.transform(X_train)\n",
    "    X_test_features = vectorizer.transform(X_test)\n",
    "    best_c, f1_score_cv = grid_search_cv(train=(X_train.reset_index()[X_train.name], \n",
    "                                                y_train.reset_index()[y_train.name]),\n",
    "                                         parameters=parameters,    \n",
    "                                         vectorizer=vectorizer,\n",
    "                                         model_fn=model_fn )\n",
    "    \n",
    "    model = model_fn(C=best_c, random_state=RANDOM_STATE, max_iter=MAX_ITER)\n",
    "    model.fit(X_train_features, y_train)\n",
    "    f1_test = f1_score(y_test, model.predict(X_test_features))\n",
    "    \n",
    "    return (model, preprocessing_fn, vectorizer, f1_score_cv, f1_test)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def none_preprocessing(data):\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_baseline = model_exploring(data_set=(data.review, data.sentiment), \n",
    "                                  preprocessing_fn=none_preprocessing, \n",
    "                                  vectorizer=CountVectorizer(), \n",
    "                                  model_fn=LogisticRegression, \n",
    "                                  parameters=hyperparams_C)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=4000,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " <function __main__.none_preprocessing(data)>,\n",
       " CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, vocabulary=None),\n",
       " 0.8909381019776237,\n",
       " 0.8994059141579334)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Preprocessing</th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>f1 cv</th>\n",
       "      <th>f1 test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                         Model  \\\n",
       "0  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "\n",
       "                                         Preprocessing  \\\n",
       "0  <function none_preprocessing at 0x0000023505146168>   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                    Vectorizer  \\\n",
       "0  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "\n",
       "    f1 cv  f1 test  \n",
       "0  0.8909   0.8994  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_data_df(models_research_df, result_baseline)\n",
    "models_research_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepocessing\n",
    "#### The first step for preprocessing is remove html tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing1 = 'Remove htmg tags'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(data_: tuple):\n",
    "    data_new = data_[0].copy()\n",
    "    data_new = data_new.apply(lambda x: re.sub(r'<br.*?>', ' ', x))\n",
    "    return (data_new, data_[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=4000,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " <function __main__.clean_html(data_: tuple)>,\n",
       " CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, vocabulary=None),\n",
       " 0.8909079366752921,\n",
       " 0.899272411721514)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_remove_html_tags = model_exploring(data_set=(data.review, data.sentiment), \n",
    "                                  preprocessing_fn=clean_html, \n",
    "                                  vectorizer=CountVectorizer(), \n",
    "                                  model_fn=LogisticRegression, \n",
    "                                  parameters=hyperparams_C)\n",
    "\n",
    "result_remove_html_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Preprocessing</th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>f1 cv</th>\n",
       "      <th>f1 test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function clean_html at 0x000002357CE3AAF8&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                         Model  \\\n",
       "0  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "1  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "\n",
       "                                         Preprocessing  \\\n",
       "0  <function none_preprocessing at 0x0000023505146168>   \n",
       "1          <function clean_html at 0x000002357CE3AAF8>   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                    Vectorizer  \\\n",
       "0  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "1  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "\n",
       "    f1 cv  f1 test  \n",
       "0  0.8909   0.8994  \n",
       "1  0.8909   0.8993  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_data_df(models_research_df, result_remove_html_tags)\n",
    "models_research_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The second step for preprocessing is remove duplicates ans incorrect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing2 = 'Remove duplicates'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(data_: tuple):\n",
    "    data_new = pd.DataFrame(columns=['X', 'y'])\n",
    "    data_new.X, data_new.y = data_ \n",
    "    data_new = data_new.drop_duplicates()\n",
    "    return (data_new.X, data_new.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=4000,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " <function __main__.remove_duplicates(data_: tuple)>,\n",
       " CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, vocabulary=None),\n",
       " 0.892155701154873,\n",
       " 0.8933955970647098)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_remove_duplicates = model_exploring(data_set=(data.review, data.sentiment), \n",
    "                                  preprocessing_fn=remove_duplicates, \n",
    "                                  vectorizer=CountVectorizer(), \n",
    "                                  model_fn=LogisticRegression, \n",
    "                                  parameters=hyperparams_C)\n",
    "result_remove_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Preprocessing</th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>f1 cv</th>\n",
       "      <th>f1 test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function clean_html at 0x000002357CE3AAF8&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function remove_duplicates at 0x0000023505460F78&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8922</td>\n",
       "      <td>0.8934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                         Model  \\\n",
       "0  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "1  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "2  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "\n",
       "                                         Preprocessing  \\\n",
       "0  <function none_preprocessing at 0x0000023505146168>   \n",
       "1          <function clean_html at 0x000002357CE3AAF8>   \n",
       "2   <function remove_duplicates at 0x0000023505460F78>   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                    Vectorizer  \\\n",
       "0  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "1  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "2  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "\n",
       "    f1 cv  f1 test  \n",
       "0  0.8909   0.8994  \n",
       "1  0.8909   0.8993  \n",
       "2  0.8922   0.8934  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_data_df(models_research_df, result_remove_duplicates)\n",
    "models_research_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The third step for preprocessing is remove digits from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing3_1 = 'Remove all digits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# firstly try removing all digits\n",
    "def clean_all_digit(data_: tuple):\n",
    "    data_new = data_[0].copy()\n",
    "    data_new = data_new.apply(lambda x: re.sub(r'\\d+', ' ', x))\n",
    "    return (data_new, data_[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=4000,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " <function __main__.clean_all_digit(data_: tuple)>,\n",
       " CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, vocabulary=None),\n",
       " 0.891342907918981,\n",
       " 0.8994461866951358)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_remove_all_digits = model_exploring(data_set=(data.review, data.sentiment), \n",
    "                                  preprocessing_fn=clean_all_digit, \n",
    "                                  vectorizer=CountVectorizer(), \n",
    "                                  model_fn=LogisticRegression, \n",
    "                                  parameters=hyperparams_C)\n",
    "result_remove_all_digits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Preprocessing</th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>f1 cv</th>\n",
       "      <th>f1 test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function clean_html at 0x000002357CE3AAF8&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function remove_duplicates at 0x0000023505460F78&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8922</td>\n",
       "      <td>0.8934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function clean_all_digit at 0x0000023505146828&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8913</td>\n",
       "      <td>0.8994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                         Model  \\\n",
       "0  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "1  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "2  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "3  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "\n",
       "                                         Preprocessing  \\\n",
       "0  <function none_preprocessing at 0x0000023505146168>   \n",
       "1          <function clean_html at 0x000002357CE3AAF8>   \n",
       "2   <function remove_duplicates at 0x0000023505460F78>   \n",
       "3     <function clean_all_digit at 0x0000023505146828>   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                    Vectorizer  \\\n",
       "0  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "1  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "2  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "3  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "\n",
       "    f1 cv  f1 test  \n",
       "0  0.8909   0.8994  \n",
       "1  0.8909   0.8993  \n",
       "2  0.8922   0.8934  \n",
       "3  0.8913   0.8994  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_data_df(models_research_df, result_remove_all_digits)\n",
    "models_research_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#secondly try splitting digits and letters\n",
    "preprocessing3_2 = 'Split digits with letters'\n",
    "\n",
    "def split_digit_letters(data_: tuple):\n",
    "    data_new = data_[0].copy()\n",
    "    data_new = data_new.apply(lambda x: re.sub(r'(\\d+)', r' \\1 ', x))\n",
    "    data_new = data_new.apply(lambda x: re.sub(r'_+', r' ', x))\n",
    "    return (data_new, data_[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=4000,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " <function __main__.split_digit_letters(data_: tuple)>,\n",
       " CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, vocabulary=None),\n",
       " 0.8909201408032728,\n",
       " 0.8998331664998331)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_split_digit_letters = model_exploring(data_set=(data.review, data.sentiment), \n",
    "                                  preprocessing_fn=split_digit_letters, \n",
    "                                  vectorizer=CountVectorizer(), \n",
    "                                  model_fn=LogisticRegression, \n",
    "                                  parameters=hyperparams_C)\n",
    "result_split_digit_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Preprocessing</th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>f1 cv</th>\n",
       "      <th>f1 test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function clean_html at 0x000002357CE3AAF8&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function remove_duplicates at 0x0000023505460F78&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8922</td>\n",
       "      <td>0.8934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function clean_all_digit at 0x0000023505146828&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8913</td>\n",
       "      <td>0.8994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function split_digit_letters at 0x0000023505146798&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                         Model  \\\n",
       "0  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "1  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "2  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "3  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "4  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "\n",
       "                                          Preprocessing  \\\n",
       "0   <function none_preprocessing at 0x0000023505146168>   \n",
       "1           <function clean_html at 0x000002357CE3AAF8>   \n",
       "2    <function remove_duplicates at 0x0000023505460F78>   \n",
       "3      <function clean_all_digit at 0x0000023505146828>   \n",
       "4  <function split_digit_letters at 0x0000023505146798>   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                    Vectorizer  \\\n",
       "0  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "1  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "2  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "3  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "4  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "\n",
       "    f1 cv  f1 test  \n",
       "0  0.8909   0.8994  \n",
       "1  0.8909   0.8993  \n",
       "2  0.8922   0.8934  \n",
       "3  0.8913   0.8994  \n",
       "4  0.8909   0.8998  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_data_df(models_research_df, result_split_digit_letters)\n",
    "models_research_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "#### The first step for vectorization is using n_gramm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=4000,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " <function __main__.none_preprocessing(data)>,\n",
       " CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, vocabulary=None),\n",
       " 0.9047562240496567,\n",
       " 0.9097819563912782)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_ngram_range_1_2 = model_exploring(data_set=(data.review, data.sentiment), \n",
    "                                  preprocessing_fn=none_preprocessing, \n",
    "                                  vectorizer=CountVectorizer(ngram_range=(1,2)), \n",
    "                                  model_fn=LogisticRegression, \n",
    "                                  parameters=hyperparams_C)\n",
    "result_ngram_range_1_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=4000,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " <function __main__.none_preprocessing(data)>,\n",
       " CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, vocabulary=None),\n",
       " 0.9039664465998424,\n",
       " 0.9093693813677831)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_ngram_range_1_3 = model_exploring(data_set=(data.review, data.sentiment), \n",
    "                                  preprocessing_fn=none_preprocessing, \n",
    "                                  vectorizer=CountVectorizer(ngram_range=(1,3)), \n",
    "                                  model_fn=LogisticRegression, \n",
    "                                  parameters=hyperparams_C)\n",
    "result_ngram_range_1_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=4000,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " <function __main__.none_preprocessing(data)>,\n",
       " CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(2, 2), preprocessor=None, stop_words=None,\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, vocabulary=None),\n",
       " 0.8876149057053941,\n",
       " 0.892662863208173)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_ngram_range_2_2 = model_exploring(data_set=(data.review, data.sentiment), \n",
    "                                  preprocessing_fn=none_preprocessing, \n",
    "                                  vectorizer=CountVectorizer(ngram_range=(2,2)), \n",
    "                                  model_fn=LogisticRegression, \n",
    "                                  parameters=hyperparams_C)\n",
    "result_ngram_range_2_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Preprocessing</th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>f1 cv</th>\n",
       "      <th>f1 test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function clean_html at 0x000002357CE3AAF8&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function remove_duplicates at 0x0000023505460F78&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8922</td>\n",
       "      <td>0.8934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function clean_all_digit at 0x0000023505146828&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8913</td>\n",
       "      <td>0.8994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function split_digit_letters at 0x0000023505146798&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 2), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.9048</td>\n",
       "      <td>0.9098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n   ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 3), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.9040</td>\n",
       "      <td>0.9094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n      ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(2, 2), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8876</td>\n",
       "      <td>0.8927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                         Model  \\\n",
       "0  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "1  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "2  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "3  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "4  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "5  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "6  LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n   ...   \n",
       "7  LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n      ...   \n",
       "\n",
       "                                          Preprocessing  \\\n",
       "0   <function none_preprocessing at 0x0000023505146168>   \n",
       "1           <function clean_html at 0x000002357CE3AAF8>   \n",
       "2    <function remove_duplicates at 0x0000023505460F78>   \n",
       "3      <function clean_all_digit at 0x0000023505146828>   \n",
       "4  <function split_digit_letters at 0x0000023505146798>   \n",
       "5   <function none_preprocessing at 0x0000023505146168>   \n",
       "6   <function none_preprocessing at 0x0000023505146168>   \n",
       "7   <function none_preprocessing at 0x0000023505146168>   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                    Vectorizer  \\\n",
       "0  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "1  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "2  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "3  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "4  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "5  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 2), preprocessor=None, stop_words=None...   \n",
       "6  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 3), preprocessor=None, stop_words=None...   \n",
       "7  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(2, 2), preprocessor=None, stop_words=None...   \n",
       "\n",
       "    f1 cv  f1 test  \n",
       "0  0.8909   0.8994  \n",
       "1  0.8909   0.8993  \n",
       "2  0.8922   0.8934  \n",
       "3  0.8913   0.8994  \n",
       "4  0.8909   0.8998  \n",
       "5  0.9048   0.9098  \n",
       "6  0.9040   0.9094  \n",
       "7  0.8876   0.8927  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_data_df(models_research_df, result_ngram_range_1_2)\n",
    "add_data_df(models_research_df, result_ngram_range_1_3)\n",
    "add_data_df(models_research_df, result_ngram_range_2_2)\n",
    "models_research_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The second step for vectorization is using the list Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=4000,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " <function __main__.none_preprocessing(data)>,\n",
       " CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, vocabulary=None),\n",
       " 0.886416323816726,\n",
       " 0.8919188560026605)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_stop_words = model_exploring(data_set=(data.review, data.sentiment), \n",
    "                                  preprocessing_fn=none_preprocessing, \n",
    "                                  vectorizer=CountVectorizer(stop_words='english'), \n",
    "                                  model_fn=LogisticRegression, \n",
    "                                  parameters=hyperparams_C)\n",
    "result_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Preprocessing</th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>f1 cv</th>\n",
       "      <th>f1 test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function clean_html at 0x000002357CE3AAF8&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function remove_duplicates at 0x0000023505460F78&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8922</td>\n",
       "      <td>0.8934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function clean_all_digit at 0x0000023505146828&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8913</td>\n",
       "      <td>0.8994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function split_digit_letters at 0x0000023505146798&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 2), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.9048</td>\n",
       "      <td>0.9098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n   ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 3), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.9040</td>\n",
       "      <td>0.9094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n      ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(2, 2), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8876</td>\n",
       "      <td>0.8927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words='eng...</td>\n",
       "      <td>0.8864</td>\n",
       "      <td>0.8919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                         Model  \\\n",
       "0  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "1  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "2  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "3  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "4  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "5  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "6  LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n   ...   \n",
       "7  LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n      ...   \n",
       "8  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "\n",
       "                                          Preprocessing  \\\n",
       "0   <function none_preprocessing at 0x0000023505146168>   \n",
       "1           <function clean_html at 0x000002357CE3AAF8>   \n",
       "2    <function remove_duplicates at 0x0000023505460F78>   \n",
       "3      <function clean_all_digit at 0x0000023505146828>   \n",
       "4  <function split_digit_letters at 0x0000023505146798>   \n",
       "5   <function none_preprocessing at 0x0000023505146168>   \n",
       "6   <function none_preprocessing at 0x0000023505146168>   \n",
       "7   <function none_preprocessing at 0x0000023505146168>   \n",
       "8   <function none_preprocessing at 0x0000023505146168>   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                    Vectorizer  \\\n",
       "0  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "1  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "2  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "3  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "4  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "5  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 2), preprocessor=None, stop_words=None...   \n",
       "6  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 3), preprocessor=None, stop_words=None...   \n",
       "7  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(2, 2), preprocessor=None, stop_words=None...   \n",
       "8  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words='eng...   \n",
       "\n",
       "    f1 cv  f1 test  \n",
       "0  0.8909   0.8994  \n",
       "1  0.8909   0.8993  \n",
       "2  0.8922   0.8934  \n",
       "3  0.8913   0.8994  \n",
       "4  0.8909   0.8998  \n",
       "5  0.9048   0.9098  \n",
       "6  0.9040   0.9094  \n",
       "7  0.8876   0.8927  \n",
       "8  0.8864   0.8919  "
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "add_data_df(models_research_df, result_stop_words)\n",
    "models_research_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The fourth step is min_df and max_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=4000,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " <function __main__.none_preprocessing(data)>,\n",
       " CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=200,\n",
       "                 ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, vocabulary=None),\n",
       " 0.8779439402218087,\n",
       " 0.8864439554577581)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_min_df = model_exploring(data_set=(data.review, data.sentiment), \n",
    "                                  preprocessing_fn=none_preprocessing, \n",
    "                                  vectorizer=CountVectorizer(min_df=200), \n",
    "                                  model_fn=LogisticRegression, \n",
    "                                  parameters=hyperparams_C)\n",
    "result_min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=4000,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " <function __main__.none_preprocessing(data)>,\n",
       " CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=0.99, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, vocabulary=None),\n",
       " 0.8908418846452433,\n",
       " 0.8985642737896494)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_max_df = model_exploring(data_set=(data.review, data.sentiment), \n",
    "                                  preprocessing_fn=none_preprocessing, \n",
    "                                  vectorizer=CountVectorizer(max_df=0.99), \n",
    "                                  model_fn=LogisticRegression, \n",
    "                                  parameters=hyperparams_C)\n",
    "result_max_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Preprocessing</th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>f1 cv</th>\n",
       "      <th>f1 test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function clean_html at 0x000002357CE3AAF8&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function remove_duplicates at 0x0000023505460F78&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8922</td>\n",
       "      <td>0.8934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function clean_all_digit at 0x0000023505146828&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8913</td>\n",
       "      <td>0.8994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function split_digit_letters at 0x0000023505146798&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 2), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.9048</td>\n",
       "      <td>0.9098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n   ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 3), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.9040</td>\n",
       "      <td>0.9094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n      ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(2, 2), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8876</td>\n",
       "      <td>0.8927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words='eng...</td>\n",
       "      <td>0.8864</td>\n",
       "      <td>0.8919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=200,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=No...</td>\n",
       "      <td>0.8779</td>\n",
       "      <td>0.8864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=0.99, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=Non...</td>\n",
       "      <td>0.8908</td>\n",
       "      <td>0.8986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                          Model  \\\n",
       "0   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "1   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "2   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "3   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "4   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "5   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "6   LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n   ...   \n",
       "7   LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n      ...   \n",
       "8   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "9   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "10  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "\n",
       "                                           Preprocessing  \\\n",
       "0    <function none_preprocessing at 0x0000023505146168>   \n",
       "1            <function clean_html at 0x000002357CE3AAF8>   \n",
       "2     <function remove_duplicates at 0x0000023505460F78>   \n",
       "3       <function clean_all_digit at 0x0000023505146828>   \n",
       "4   <function split_digit_letters at 0x0000023505146798>   \n",
       "5    <function none_preprocessing at 0x0000023505146168>   \n",
       "6    <function none_preprocessing at 0x0000023505146168>   \n",
       "7    <function none_preprocessing at 0x0000023505146168>   \n",
       "8    <function none_preprocessing at 0x0000023505146168>   \n",
       "9    <function none_preprocessing at 0x0000023505146168>   \n",
       "10   <function none_preprocessing at 0x0000023505146168>   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     Vectorizer  \\\n",
       "0   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "1   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "2   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "3   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "4   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "5   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 2), preprocessor=None, stop_words=None...   \n",
       "6   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 3), preprocessor=None, stop_words=None...   \n",
       "7   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(2, 2), preprocessor=None, stop_words=None...   \n",
       "8   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words='eng...   \n",
       "9   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=200,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=No...   \n",
       "10  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=0.99, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=Non...   \n",
       "\n",
       "     f1 cv  f1 test  \n",
       "0   0.8909   0.8994  \n",
       "1   0.8909   0.8993  \n",
       "2   0.8922   0.8934  \n",
       "3   0.8913   0.8994  \n",
       "4   0.8909   0.8998  \n",
       "5   0.9048   0.9098  \n",
       "6   0.9040   0.9094  \n",
       "7   0.8876   0.8927  \n",
       "8   0.8864   0.8919  \n",
       "9   0.8779   0.8864  \n",
       "10  0.8908   0.8986  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_data_df(models_research_df, result_min_df)\n",
    "add_data_df(models_research_df, result_max_df)\n",
    "models_research_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfIdfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply TfidfVectorizer with parameters:\n",
    "smooth_idfbool = False, in our set the situation when an extra document was seen containing every term in the collection exactly once is unlikely.\n",
    "sublinear_tf=True, replace tf with 1 + log(tf).(this parameter might be used for gridSearch)\n",
    "norm=None, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=4000,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " <function __main__.none_preprocessing(data)>,\n",
       " TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                 sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, use_idf=True, vocabulary=None),\n",
       " 0.8957970597601463,\n",
       " 0.9047714114593731)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_tfidf = model_exploring(data_set=(data.review, data.sentiment), \n",
    "                                  preprocessing_fn=none_preprocessing, \n",
    "                                  vectorizer=TfidfVectorizer(), \n",
    "                                  model_fn=LogisticRegression, \n",
    "                                  parameters=hyperparams_C)\n",
    "result_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Preprocessing</th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>f1 cv</th>\n",
       "      <th>f1 test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function clean_html at 0x000002357CE3AAF8&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function remove_duplicates at 0x0000023505460F78&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8922</td>\n",
       "      <td>0.8934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function clean_all_digit at 0x0000023505146828&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8913</td>\n",
       "      <td>0.8994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function split_digit_letters at 0x0000023505146798&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 2), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.9048</td>\n",
       "      <td>0.9098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n   ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 3), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.9040</td>\n",
       "      <td>0.9094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n      ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(2, 2), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8876</td>\n",
       "      <td>0.8927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words='eng...</td>\n",
       "      <td>0.8864</td>\n",
       "      <td>0.8919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=200,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=No...</td>\n",
       "      <td>0.8779</td>\n",
       "      <td>0.8864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=0.99, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=Non...</td>\n",
       "      <td>0.8908</td>\n",
       "      <td>0.8986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n     ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.float64'&gt;, encoding='utf-8',\\n                input='content', lowercase=True, max_df=1.0, max_features=None,\\n                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\\n ...</td>\n",
       "      <td>0.8958</td>\n",
       "      <td>0.9048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                          Model  \\\n",
       "0   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "1   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "2   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "3   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "4   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "5   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "6   LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n   ...   \n",
       "7   LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n      ...   \n",
       "8   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "9   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "10  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "11  LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n     ...   \n",
       "\n",
       "                                           Preprocessing  \\\n",
       "0    <function none_preprocessing at 0x0000023505146168>   \n",
       "1            <function clean_html at 0x000002357CE3AAF8>   \n",
       "2     <function remove_duplicates at 0x0000023505460F78>   \n",
       "3       <function clean_all_digit at 0x0000023505146828>   \n",
       "4   <function split_digit_letters at 0x0000023505146798>   \n",
       "5    <function none_preprocessing at 0x0000023505146168>   \n",
       "6    <function none_preprocessing at 0x0000023505146168>   \n",
       "7    <function none_preprocessing at 0x0000023505146168>   \n",
       "8    <function none_preprocessing at 0x0000023505146168>   \n",
       "9    <function none_preprocessing at 0x0000023505146168>   \n",
       "10   <function none_preprocessing at 0x0000023505146168>   \n",
       "11   <function none_preprocessing at 0x0000023505146168>   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     Vectorizer  \\\n",
       "0   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "1   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "2   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "3   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "4   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "5   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 2), preprocessor=None, stop_words=None...   \n",
       "6   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 3), preprocessor=None, stop_words=None...   \n",
       "7   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(2, 2), preprocessor=None, stop_words=None...   \n",
       "8   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words='eng...   \n",
       "9   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=200,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=No...   \n",
       "10  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=0.99, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=Non...   \n",
       "11  TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.float64'>, encoding='utf-8',\\n                input='content', lowercase=True, max_df=1.0, max_features=None,\\n                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\\n ...   \n",
       "\n",
       "     f1 cv  f1 test  \n",
       "0   0.8909   0.8994  \n",
       "1   0.8909   0.8993  \n",
       "2   0.8922   0.8934  \n",
       "3   0.8913   0.8994  \n",
       "4   0.8909   0.8998  \n",
       "5   0.9048   0.9098  \n",
       "6   0.9040   0.9094  \n",
       "7   0.8876   0.8927  \n",
       "8   0.8864   0.8919  \n",
       "9   0.8779   0.8864  \n",
       "10  0.8908   0.8986  \n",
       "11  0.8958   0.9048  "
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_data_df(models_research_df, result_tfidf)\n",
    "models_research_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=10000, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=4000,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " <function __main__.none_preprocessing(data)>,\n",
       " TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                 sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, use_idf=True, vocabulary=None),\n",
       " 0.9139108669635979,\n",
       " 0.9179084619478446)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_tfidf_ngramm_1_2 = model_exploring(data_set=(data.review, data.sentiment), \n",
    "                                  preprocessing_fn=none_preprocessing, \n",
    "                                  vectorizer=TfidfVectorizer(ngram_range=(1,2)), \n",
    "                                  model_fn=LogisticRegression, \n",
    "                                  parameters=hyperparams_C)\n",
    "result_tfidf_ngramm_1_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Preprocessing</th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>f1 cv</th>\n",
       "      <th>f1 test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function clean_html at 0x000002357CE3AAF8&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function remove_duplicates at 0x0000023505460F78&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8922</td>\n",
       "      <td>0.8934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function clean_all_digit at 0x0000023505146828&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8913</td>\n",
       "      <td>0.8994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function split_digit_letters at 0x0000023505146798&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 2), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.9048</td>\n",
       "      <td>0.9098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n   ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 3), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.9040</td>\n",
       "      <td>0.9094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n      ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(2, 2), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8876</td>\n",
       "      <td>0.8927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words='eng...</td>\n",
       "      <td>0.8864</td>\n",
       "      <td>0.8919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=200,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=No...</td>\n",
       "      <td>0.8779</td>\n",
       "      <td>0.8864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=0.99, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=Non...</td>\n",
       "      <td>0.8908</td>\n",
       "      <td>0.8986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n     ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.float64'&gt;, encoding='utf-8',\\n                input='content', lowercase=True, max_df=1.0, max_features=None,\\n                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\\n ...</td>\n",
       "      <td>0.8958</td>\n",
       "      <td>0.9048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(C=10000, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n  ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.float64'&gt;, encoding='utf-8',\\n                input='content', lowercase=True, max_df=1.0, max_features=None,\\n                min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\\n ...</td>\n",
       "      <td>0.9139</td>\n",
       "      <td>0.9179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                          Model  \\\n",
       "0   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "1   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "2   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "3   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "4   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "5   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "6   LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n   ...   \n",
       "7   LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n      ...   \n",
       "8   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "9   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "10  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "11  LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n     ...   \n",
       "12  LogisticRegression(C=10000, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n  ...   \n",
       "\n",
       "                                           Preprocessing  \\\n",
       "0    <function none_preprocessing at 0x0000023505146168>   \n",
       "1            <function clean_html at 0x000002357CE3AAF8>   \n",
       "2     <function remove_duplicates at 0x0000023505460F78>   \n",
       "3       <function clean_all_digit at 0x0000023505146828>   \n",
       "4   <function split_digit_letters at 0x0000023505146798>   \n",
       "5    <function none_preprocessing at 0x0000023505146168>   \n",
       "6    <function none_preprocessing at 0x0000023505146168>   \n",
       "7    <function none_preprocessing at 0x0000023505146168>   \n",
       "8    <function none_preprocessing at 0x0000023505146168>   \n",
       "9    <function none_preprocessing at 0x0000023505146168>   \n",
       "10   <function none_preprocessing at 0x0000023505146168>   \n",
       "11   <function none_preprocessing at 0x0000023505146168>   \n",
       "12   <function none_preprocessing at 0x0000023505146168>   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     Vectorizer  \\\n",
       "0   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "1   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "2   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "3   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "4   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "5   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 2), preprocessor=None, stop_words=None...   \n",
       "6   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 3), preprocessor=None, stop_words=None...   \n",
       "7   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(2, 2), preprocessor=None, stop_words=None...   \n",
       "8   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words='eng...   \n",
       "9   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=200,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=No...   \n",
       "10  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=0.99, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=Non...   \n",
       "11  TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.float64'>, encoding='utf-8',\\n                input='content', lowercase=True, max_df=1.0, max_features=None,\\n                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\\n ...   \n",
       "12  TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.float64'>, encoding='utf-8',\\n                input='content', lowercase=True, max_df=1.0, max_features=None,\\n                min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\\n ...   \n",
       "\n",
       "     f1 cv  f1 test  \n",
       "0   0.8909   0.8994  \n",
       "1   0.8909   0.8993  \n",
       "2   0.8922   0.8934  \n",
       "3   0.8913   0.8994  \n",
       "4   0.8909   0.8998  \n",
       "5   0.9048   0.9098  \n",
       "6   0.9040   0.9094  \n",
       "7   0.8876   0.8927  \n",
       "8   0.8864   0.8919  \n",
       "9   0.8779   0.8864  \n",
       "10  0.8908   0.8986  \n",
       "11  0.8958   0.9048  \n",
       "12  0.9139   0.9179  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_data_df(models_research_df, result_tfidf_ngramm_1_2)\n",
    "models_research_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply the preprocessing steps and the best vectorizer  for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data_: tuple):\n",
    "    return clean_html(data_=remove_duplicates(data_=split_digit_letters(data_=data_)))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=10000, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=4000,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " <function __main__.preprocessing(data_: tuple)>,\n",
       " TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                 sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, use_idf=True, vocabulary=None),\n",
       " 0.9153568213187256,\n",
       " 0.9127052722558341)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_model = model_exploring(data_set=(data.review, data.sentiment), \n",
    "                                  preprocessing_fn=preprocessing, \n",
    "                                  vectorizer=TfidfVectorizer(ngram_range=(1,2)), \n",
    "                                  model_fn=LogisticRegression, \n",
    "                                  parameters=hyperparams_C)\n",
    "result_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Preprocessing</th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>f1 cv</th>\n",
       "      <th>f1 test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function clean_html at 0x000002357CE3AAF8&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function remove_duplicates at 0x0000023505460F78&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8922</td>\n",
       "      <td>0.8934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function clean_all_digit at 0x0000023505146828&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8913</td>\n",
       "      <td>0.8994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function split_digit_letters at 0x0000023505146798&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 2), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.9048</td>\n",
       "      <td>0.9098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n   ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 3), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.9040</td>\n",
       "      <td>0.9094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n      ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(2, 2), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8876</td>\n",
       "      <td>0.8927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words='eng...</td>\n",
       "      <td>0.8864</td>\n",
       "      <td>0.8919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=200,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=No...</td>\n",
       "      <td>0.8779</td>\n",
       "      <td>0.8864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=0.99, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=Non...</td>\n",
       "      <td>0.8908</td>\n",
       "      <td>0.8986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n     ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.float64'&gt;, encoding='utf-8',\\n                input='content', lowercase=True, max_df=1.0, max_features=None,\\n                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\\n ...</td>\n",
       "      <td>0.8958</td>\n",
       "      <td>0.9048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(C=10000, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n  ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.float64'&gt;, encoding='utf-8',\\n                input='content', lowercase=True, max_df=1.0, max_features=None,\\n                min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\\n ...</td>\n",
       "      <td>0.9139</td>\n",
       "      <td>0.9179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(C=10000, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n  ...</td>\n",
       "      <td>&lt;function preprocessing at 0x000002351115ADC8&gt;</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.float64'&gt;, encoding='utf-8',\\n                input='content', lowercase=True, max_df=1.0, max_features=None,\\n                min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\\n ...</td>\n",
       "      <td>0.9154</td>\n",
       "      <td>0.9127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                          Model  \\\n",
       "0   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "1   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "2   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "3   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "4   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "5   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "6   LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n   ...   \n",
       "7   LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n      ...   \n",
       "8   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "9   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "10  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "11  LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n     ...   \n",
       "12  LogisticRegression(C=10000, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n  ...   \n",
       "13  LogisticRegression(C=10000, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n  ...   \n",
       "\n",
       "                                           Preprocessing  \\\n",
       "0    <function none_preprocessing at 0x0000023505146168>   \n",
       "1            <function clean_html at 0x000002357CE3AAF8>   \n",
       "2     <function remove_duplicates at 0x0000023505460F78>   \n",
       "3       <function clean_all_digit at 0x0000023505146828>   \n",
       "4   <function split_digit_letters at 0x0000023505146798>   \n",
       "5    <function none_preprocessing at 0x0000023505146168>   \n",
       "6    <function none_preprocessing at 0x0000023505146168>   \n",
       "7    <function none_preprocessing at 0x0000023505146168>   \n",
       "8    <function none_preprocessing at 0x0000023505146168>   \n",
       "9    <function none_preprocessing at 0x0000023505146168>   \n",
       "10   <function none_preprocessing at 0x0000023505146168>   \n",
       "11   <function none_preprocessing at 0x0000023505146168>   \n",
       "12   <function none_preprocessing at 0x0000023505146168>   \n",
       "13        <function preprocessing at 0x000002351115ADC8>   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     Vectorizer  \\\n",
       "0   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "1   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "2   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "3   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "4   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "5   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 2), preprocessor=None, stop_words=None...   \n",
       "6   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 3), preprocessor=None, stop_words=None...   \n",
       "7   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(2, 2), preprocessor=None, stop_words=None...   \n",
       "8   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words='eng...   \n",
       "9   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=200,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=No...   \n",
       "10  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=0.99, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=Non...   \n",
       "11  TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.float64'>, encoding='utf-8',\\n                input='content', lowercase=True, max_df=1.0, max_features=None,\\n                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\\n ...   \n",
       "12  TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.float64'>, encoding='utf-8',\\n                input='content', lowercase=True, max_df=1.0, max_features=None,\\n                min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\\n ...   \n",
       "13  TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.float64'>, encoding='utf-8',\\n                input='content', lowercase=True, max_df=1.0, max_features=None,\\n                min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\\n ...   \n",
       "\n",
       "     f1 cv  f1 test  \n",
       "0   0.8909   0.8994  \n",
       "1   0.8909   0.8993  \n",
       "2   0.8922   0.8934  \n",
       "3   0.8913   0.8994  \n",
       "4   0.8909   0.8998  \n",
       "5   0.9048   0.9098  \n",
       "6   0.9040   0.9094  \n",
       "7   0.8876   0.8927  \n",
       "8   0.8864   0.8919  \n",
       "9   0.8779   0.8864  \n",
       "10  0.8908   0.8986  \n",
       "11  0.8958   0.9048  \n",
       "12  0.9139   0.9179  \n",
       "13  0.9154   0.9127  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_data_df(models_research_df, result_model)\n",
    "models_research_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_f1_curve(title='',\n",
    "             lines=(models_research_df['f1 cv'], models_research_df['f1 test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
