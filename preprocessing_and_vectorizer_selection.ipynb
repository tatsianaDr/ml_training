{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Problem Undestanding\n",
    "Here we will look at a Data Science challenge within the IMDB space. For our model fitting choose the f1-score metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries & data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters for display dataframe\n",
    "pd.set_option('precision', 4)\n",
    "pd.set_option('max_colwidth', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definition constants\n",
    "RANDOM_STATE = 11\n",
    "MAX_ITER=4000\n",
    "NUMBER_K_FOLD = 5\n",
    "TARGET_METRIC = 'f1'\n",
    "TEST_SIZE = 0.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Preprocessing</th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>f1 cv</th>\n",
       "      <th>f1 test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Model, Preprocessing, Vectorizer, f1 cv, f1 test]\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Results of exploring for model we will provided in table model_search_result\n",
    "models_research_df = pd.DataFrame(columns=['Model',\n",
    "                                            'Preprocessing',\n",
    "                                            'Vectorizer',\n",
    "                                            'f1 cv',\n",
    "                                            'f1 test', ], )\n",
    "models_research_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add data to result research dataframe\n",
    "def add_data_df(table_df_for_add, model_research: tuple):\n",
    "    if any([list(row.values ) == list(model_research) for _, row in table_df_for_add.iterrows()]):\n",
    "        return\n",
    "    table_df_for_add.loc[len(table_df_for_add)] = model_research\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_f1_curve(title: str, lines: tuple):\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.title(title)\n",
    "    for line in lines:\n",
    "        plt.plot(range(0, len(line)),line, label=line.name)\n",
    "    plt.xlabel('Index the model from the result  table')\n",
    "    plt.ylabel('f1')\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import & display data\n",
    "data = pd.read_csv('data/IMDB_Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data\n",
    "Our training set has 50K movie reviews for natural language processing.  This is a dataser for binary sentiment classification. \n",
    "For more dataset information, please go through the following link,\n",
    "https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.&lt;br /&gt;&lt;br /&gt;The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO....</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. &lt;br /&gt;&lt;br /&gt;The actors are extremely well chosen- Michael Sheen not only \"has got all the pola...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet &amp; his parents are fighting all the time.&lt;br /&gt;&lt;br /&gt;This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.&lt;br /&gt;&lt;br /&gt;OK, first of all when you're going t...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what money, power and success do to people in the different situations we encounter. &lt;br /&gt;&lt;br /&gt;This being a...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                        review  \\\n",
       "0  One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO....   \n",
       "1  A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"has got all the pola...   \n",
       "2  I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may...   \n",
       "3  Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.<br /><br />OK, first of all when you're going t...   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what money, power and success do to people in the different situations we encounter. <br /><br />This being a...   \n",
       "\n",
       "  sentiment  \n",
       "0  positive  \n",
       "1  positive  \n",
       "2  positive  \n",
       "3  negative  \n",
       "4  positive  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review       50000\n",
       "sentiment    50000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>49582</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Loved today's show!!! It was a variety and not solely cooking (which would have been great too). Very stimulating and captivating, always keeping the viewer peeking around the corner to see what was coming up next. She is as down to earth and as personable as you get, like one of us which made t...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                             review  \\\n",
       "count                                                                                                                                                                                                                                                                                                         50000   \n",
       "unique                                                                                                                                                                                                                                                                                                        49582   \n",
       "top     Loved today's show!!! It was a variety and not solely cooking (which would have been great too). Very stimulating and captivating, always keeping the viewer peeking around the corner to see what was coming up next. She is as down to earth and as personable as you get, like one of us which made t...   \n",
       "freq                                                                                                                                                                                                                                                                                                              5   \n",
       "\n",
       "       sentiment  \n",
       "count      50000  \n",
       "unique         2  \n",
       "top     negative  \n",
       "freq       25000  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataset contains invalid non-unique values. In the next step research we should drop all data repetitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3537</th>\n",
       "      <td>Quite what the producers of this appalling adaptation were trying to do is impossible to fathom.&lt;br /&gt;&lt;br /&gt;A group of top quality actors, in the main well cast (with a couple of notable exceptions), who give pretty good performances. Penelope Keith is perfect as Aunt Louise and equally good is ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3769</th>\n",
       "      <td>My favourite police series of all time turns to a TV-film. Does it work? Yes. Gee runs for mayor and gets shot. The Homicide \"hall of fame\" turns up. Pembleton and nearly all of the cops who ever played in this series. A lot of flashbacks helps you who hasn´t seen the TV-series but it amuses the...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4391</th>\n",
       "      <td>Beautiful film, pure Cassavetes style. Gena Rowland gives a stunning performance of a declining actress, dealing with success, aging, loneliness...and alcoholism. She tries to escape her own subconscious ghosts, embodied by the death spectre of a young girl. Acceptance of oneself, of human condi...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6352</th>\n",
       "      <td>If you liked the Grinch movie... go watch that again, because this was no where near as good a Seussian movie translation. Mike Myers' Cat is probably the most annoying character to \"grace\" the screen in recent times. His voice/accent is terrible and he laughs at his own jokes with an awful weas...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6479</th>\n",
       "      <td>I want very much to believe that the above quote (specifically, the English subtitle translation), which was actually written, not spoken, in a rejection letter a publisher sends to the protagonist, was meant to be self-referential in a tongue-in-cheek manner. But if so, director Leos Carax appa...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                           review  \\\n",
       "3537  Quite what the producers of this appalling adaptation were trying to do is impossible to fathom.<br /><br />A group of top quality actors, in the main well cast (with a couple of notable exceptions), who give pretty good performances. Penelope Keith is perfect as Aunt Louise and equally good is ...   \n",
       "3769  My favourite police series of all time turns to a TV-film. Does it work? Yes. Gee runs for mayor and gets shot. The Homicide \"hall of fame\" turns up. Pembleton and nearly all of the cops who ever played in this series. A lot of flashbacks helps you who hasn´t seen the TV-series but it amuses the...   \n",
       "4391  Beautiful film, pure Cassavetes style. Gena Rowland gives a stunning performance of a declining actress, dealing with success, aging, loneliness...and alcoholism. She tries to escape her own subconscious ghosts, embodied by the death spectre of a young girl. Acceptance of oneself, of human condi...   \n",
       "6352  If you liked the Grinch movie... go watch that again, because this was no where near as good a Seussian movie translation. Mike Myers' Cat is probably the most annoying character to \"grace\" the screen in recent times. His voice/accent is terrible and he laughs at his own jokes with an awful weas...   \n",
       "6479  I want very much to believe that the above quote (specifically, the English subtitle translation), which was actually written, not spoken, in a rejection letter a publisher sends to the protagonist, was meant to be self-referential in a tongue-in-cheek manner. But if so, director Leos Carax appa...   \n",
       "\n",
       "     sentiment  \n",
       "3537  negative  \n",
       "3769  positive  \n",
       "4391  positive  \n",
       "6352  negative  \n",
       "6479  negative  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.duplicated()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### replace the categoric values from 'sentiment' to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sentiment'] = data['sentiment'].replace({'positive' : 1, 'negative' : 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.&lt;br /&gt;&lt;br /&gt;The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. &lt;br /&gt;&lt;br /&gt;The actors are extremely well chosen- Michael Sheen not only \"has got all the pola...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet &amp; his parents are fighting all the time.&lt;br /&gt;&lt;br /&gt;This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.&lt;br /&gt;&lt;br /&gt;OK, first of all when you're going t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what money, power and success do to people in the different situations we encounter. &lt;br /&gt;&lt;br /&gt;This being a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                        review  \\\n",
       "0  One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO....   \n",
       "1  A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"has got all the pola...   \n",
       "2  I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may...   \n",
       "3  Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.<br /><br />OK, first of all when you're going t...   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what money, power and success do to people in the different situations we encounter. <br /><br />This being a...   \n",
       "\n",
       "   sentiment  \n",
       "0          1  \n",
       "1          1  \n",
       "2          1  \n",
       "3          0  \n",
       "4          1  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Explore models with preprocessing and vectoriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start work with Logistic Regression for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams_C = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "\n",
    "#function for find out the samples with particular text\n",
    "def find_out_samples_with_text(data_for_explore, template):\n",
    "    return [review for review in data_for_explore['review'] if  template in review.lower()]\n",
    "\n",
    "#function for exploring data and fit model\n",
    "def grid_search_cv(train: tuple, parameters: list, vectorizer, model_fn):\n",
    "    kf = KFold(n_splits=NUMBER_K_FOLD)\n",
    "    \n",
    "    gs_metrics = {}\n",
    "    for parameter in parameters:\n",
    "        cv_metrics = []\n",
    "        \n",
    "        folds = kf.split(train[0])\n",
    "        for train_index, val_index in folds:\n",
    "            fold_X_train, fold_X_val = train[0][train_index], train[0][val_index]\n",
    "            fold_y_train, fold_y_val = train[1][train_index], train[1][val_index]\n",
    "            \n",
    "            vectorizer.fit(fold_X_train)\n",
    "            fold_X_train_features = vectorizer.transform(fold_X_train)\n",
    "            \n",
    "            fold_X_val_features = vectorizer.transform(fold_X_val)\n",
    "            \n",
    "            model = model_fn(C=parameter, random_state=RANDOM_STATE, max_iter=MAX_ITER)\n",
    "            model.fit(fold_X_train_features, fold_y_train)\n",
    "\n",
    "            metric = f1_score(model.predict(fold_X_val_features), fold_y_val)\n",
    "            cv_metrics.append(metric)\n",
    "            \n",
    "            cv_score = np.mean(cv_metrics)\n",
    "            \n",
    "            gs_metrics[parameter] = cv_score\n",
    "    return sorted(gs_metrics.items(), key=lambda x: x[1], reverse=True)[0]\n",
    "\n",
    "def model_exploring(data_set: tuple, preprocessing_fn, vectorizer, model_fn, parameters: list):\n",
    "    data_set = preprocessing_fn(data_set)\n",
    "    X = data_set[0]\n",
    "    y = data_set[1]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                        y,\n",
    "                                                        test_size=TEST_SIZE, \n",
    "                                                        random_state=RANDOM_STATE, \n",
    "                                                        stratify = y)\n",
    "    X_train_features = vectorizer.fit(X_train)\n",
    "    X_train_features = vectorizer.transform(X_train)\n",
    "    X_test_features = vectorizer.transform(X_test)\n",
    "    best_c, f1_score_cv = grid_search_cv(train=(X_train.reset_index()[X_train.name], \n",
    "                                                y_train.reset_index()[y_train.name]),\n",
    "                                         parameters=parameters,    \n",
    "                                         vectorizer=vectorizer,\n",
    "                                         model_fn=model_fn )\n",
    "    \n",
    "    model = model_fn(C=best_c, random_state=RANDOM_STATE, max_iter=MAX_ITER)\n",
    "    model.fit(X_train_features, y_train)\n",
    "    f1_test = f1_score(y_test, model.predict(X_test_features))\n",
    "    \n",
    "    return (model, preprocessing_fn, vectorizer, f1_score_cv, f1_test)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def none_preprocessing(data):\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_baseline = model_exploring(data_set=(data.review, data.sentiment), \n",
    "                                  preprocessing_fn=none_preprocessing, \n",
    "                                  vectorizer=CountVectorizer(), \n",
    "                                  model_fn=LogisticRegression, \n",
    "                                  parameters=hyperparams_C)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=4000,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " <function __main__.none_preprocessing(data)>,\n",
       " CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, vocabulary=None),\n",
       " 0.8909381019776237,\n",
       " 0.8994059141579334)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Preprocessing</th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>f1 cv</th>\n",
       "      <th>f1 test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                         Model  \\\n",
       "0  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "\n",
       "                                         Preprocessing  \\\n",
       "0  <function none_preprocessing at 0x0000023505146168>   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                    Vectorizer  \\\n",
       "0  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "\n",
       "    f1 cv  f1 test  \n",
       "0  0.8909   0.8994  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_data_df(models_research_df, result_baseline)\n",
    "models_research_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepocessing\n",
    "#### The first step for preprocessing is remove html tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing1 = 'Remove htmg tags'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(data_: tuple):\n",
    "    data_new = data_[0].copy()\n",
    "    data_new = data_new.apply(lambda x: re.sub(r'<br.*?>', ' ', x))\n",
    "    return (data_new, data_[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=4000,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " <function __main__.clean_html(data_: tuple)>,\n",
       " CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, vocabulary=None),\n",
       " 0.8909079366752921,\n",
       " 0.899272411721514)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_remove_html_tags = model_exploring(data_set=(data.review, data.sentiment), \n",
    "                                  preprocessing_fn=clean_html, \n",
    "                                  vectorizer=CountVectorizer(), \n",
    "                                  model_fn=LogisticRegression, \n",
    "                                  parameters=hyperparams_C)\n",
    "\n",
    "result_remove_html_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Preprocessing</th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>f1 cv</th>\n",
       "      <th>f1 test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function clean_html at 0x000002357CE3AAF8&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                         Model  \\\n",
       "0  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "1  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "\n",
       "                                         Preprocessing  \\\n",
       "0  <function none_preprocessing at 0x0000023505146168>   \n",
       "1          <function clean_html at 0x000002357CE3AAF8>   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                    Vectorizer  \\\n",
       "0  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "1  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "\n",
       "    f1 cv  f1 test  \n",
       "0  0.8909   0.8994  \n",
       "1  0.8909   0.8993  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_data_df(models_research_df, result_remove_html_tags)\n",
    "models_research_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The second step for preprocessing is remove duplicates ans incorrect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing2 = 'Remove duplicates'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(data_: tuple):\n",
    "    data_new = pd.DataFrame(columns=['X', 'y'])\n",
    "    data_new.X, data_new.y = data_ \n",
    "    data_new = data_new.drop_duplicates()\n",
    "    return (data_new.X, data_new.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=4000,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " <function __main__.remove_duplicates(data_: tuple)>,\n",
       " CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, vocabulary=None),\n",
       " 0.892155701154873,\n",
       " 0.8933955970647098)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_remove_duplicates = model_exploring(data_set=(data.review, data.sentiment), \n",
    "                                  preprocessing_fn=remove_duplicates, \n",
    "                                  vectorizer=CountVectorizer(), \n",
    "                                  model_fn=LogisticRegression, \n",
    "                                  parameters=hyperparams_C)\n",
    "result_remove_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Preprocessing</th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>f1 cv</th>\n",
       "      <th>f1 test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function clean_html at 0x000002357CE3AAF8&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function remove_duplicates at 0x0000023505460F78&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8922</td>\n",
       "      <td>0.8934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                         Model  \\\n",
       "0  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "1  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "2  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "\n",
       "                                         Preprocessing  \\\n",
       "0  <function none_preprocessing at 0x0000023505146168>   \n",
       "1          <function clean_html at 0x000002357CE3AAF8>   \n",
       "2   <function remove_duplicates at 0x0000023505460F78>   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                    Vectorizer  \\\n",
       "0  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "1  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "2  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "\n",
       "    f1 cv  f1 test  \n",
       "0  0.8909   0.8994  \n",
       "1  0.8909   0.8993  \n",
       "2  0.8922   0.8934  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_data_df(models_research_df, result_remove_duplicates)\n",
    "models_research_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The third step for preprocessing is remove digits from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing3_1 = 'Remove all digits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# firstly try removing all digits\n",
    "def clean_all_digit(data_: tuple):\n",
    "    data_new = data_[0].copy()\n",
    "    data_new = data_new.apply(lambda x: re.sub(r'\\d+', ' ', x))\n",
    "    return (data_new, data_[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=4000,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " <function __main__.clean_all_digit(data_: tuple)>,\n",
       " CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, vocabulary=None),\n",
       " 0.891342907918981,\n",
       " 0.8994461866951358)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_remove_all_digits = model_exploring(data_set=(data.review, data.sentiment), \n",
    "                                  preprocessing_fn=clean_all_digit, \n",
    "                                  vectorizer=CountVectorizer(), \n",
    "                                  model_fn=LogisticRegression, \n",
    "                                  parameters=hyperparams_C)\n",
    "result_remove_all_digits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Preprocessing</th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>f1 cv</th>\n",
       "      <th>f1 test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function clean_html at 0x000002357CE3AAF8&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function remove_duplicates at 0x0000023505460F78&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8922</td>\n",
       "      <td>0.8934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function clean_all_digit at 0x0000023505146828&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8913</td>\n",
       "      <td>0.8994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                         Model  \\\n",
       "0  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "1  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "2  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "3  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "\n",
       "                                         Preprocessing  \\\n",
       "0  <function none_preprocessing at 0x0000023505146168>   \n",
       "1          <function clean_html at 0x000002357CE3AAF8>   \n",
       "2   <function remove_duplicates at 0x0000023505460F78>   \n",
       "3     <function clean_all_digit at 0x0000023505146828>   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                    Vectorizer  \\\n",
       "0  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "1  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "2  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "3  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "\n",
       "    f1 cv  f1 test  \n",
       "0  0.8909   0.8994  \n",
       "1  0.8909   0.8993  \n",
       "2  0.8922   0.8934  \n",
       "3  0.8913   0.8994  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_data_df(models_research_df, result_remove_all_digits)\n",
    "models_research_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#secondly try splitting digits and letters\n",
    "preprocessing3_2 = 'Split digits with letters'\n",
    "\n",
    "def split_digit_letters(data_: tuple):\n",
    "    data_new = data_[0].copy()\n",
    "    data_new = data_new.apply(lambda x: re.sub(r'(\\d+)', r' \\1 ', x))\n",
    "    data_new = data_new.apply(lambda x: re.sub(r'_+', r' ', x))\n",
    "    return (data_new, data_[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=4000,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " <function __main__.split_digit_letters(data_: tuple)>,\n",
       " CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, vocabulary=None),\n",
       " 0.8909201408032728,\n",
       " 0.8998331664998331)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_split_digit_letters = model_exploring(data_set=(data.review, data.sentiment), \n",
    "                                  preprocessing_fn=split_digit_letters, \n",
    "                                  vectorizer=CountVectorizer(), \n",
    "                                  model_fn=LogisticRegression, \n",
    "                                  parameters=hyperparams_C)\n",
    "result_split_digit_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Preprocessing</th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>f1 cv</th>\n",
       "      <th>f1 test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function clean_html at 0x000002357CE3AAF8&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function remove_duplicates at 0x0000023505460F78&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8922</td>\n",
       "      <td>0.8934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function clean_all_digit at 0x0000023505146828&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8913</td>\n",
       "      <td>0.8994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function split_digit_letters at 0x0000023505146798&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                         Model  \\\n",
       "0  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "1  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "2  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "3  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "4  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "\n",
       "                                          Preprocessing  \\\n",
       "0   <function none_preprocessing at 0x0000023505146168>   \n",
       "1           <function clean_html at 0x000002357CE3AAF8>   \n",
       "2    <function remove_duplicates at 0x0000023505460F78>   \n",
       "3      <function clean_all_digit at 0x0000023505146828>   \n",
       "4  <function split_digit_letters at 0x0000023505146798>   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                    Vectorizer  \\\n",
       "0  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "1  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "2  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "3  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "4  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "\n",
       "    f1 cv  f1 test  \n",
       "0  0.8909   0.8994  \n",
       "1  0.8909   0.8993  \n",
       "2  0.8922   0.8934  \n",
       "3  0.8913   0.8994  \n",
       "4  0.8909   0.8998  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_data_df(models_research_df, result_split_digit_letters)\n",
    "models_research_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "#### The first step for vectorization is using n_gramm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=4000,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " <function __main__.none_preprocessing(data)>,\n",
       " CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, vocabulary=None),\n",
       " 0.9047562240496567,\n",
       " 0.9097819563912782)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_ngram_range_1_2 = model_exploring(data_set=(data.review, data.sentiment), \n",
    "                                  preprocessing_fn=none_preprocessing, \n",
    "                                  vectorizer=CountVectorizer(ngram_range=(1,2)), \n",
    "                                  model_fn=LogisticRegression, \n",
    "                                  parameters=hyperparams_C)\n",
    "result_ngram_range_1_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=4000,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " <function __main__.none_preprocessing(data)>,\n",
       " CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, vocabulary=None),\n",
       " 0.9039664465998424,\n",
       " 0.9093693813677831)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_ngram_range_1_3 = model_exploring(data_set=(data.review, data.sentiment), \n",
    "                                  preprocessing_fn=none_preprocessing, \n",
    "                                  vectorizer=CountVectorizer(ngram_range=(1,3)), \n",
    "                                  model_fn=LogisticRegression, \n",
    "                                  parameters=hyperparams_C)\n",
    "result_ngram_range_1_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=4000,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " <function __main__.none_preprocessing(data)>,\n",
       " CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(2, 2), preprocessor=None, stop_words=None,\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, vocabulary=None),\n",
       " 0.8876149057053941,\n",
       " 0.892662863208173)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_ngram_range_2_2 = model_exploring(data_set=(data.review, data.sentiment), \n",
    "                                  preprocessing_fn=none_preprocessing, \n",
    "                                  vectorizer=CountVectorizer(ngram_range=(2,2)), \n",
    "                                  model_fn=LogisticRegression, \n",
    "                                  parameters=hyperparams_C)\n",
    "result_ngram_range_2_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Preprocessing</th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>f1 cv</th>\n",
       "      <th>f1 test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function clean_html at 0x000002357CE3AAF8&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function remove_duplicates at 0x0000023505460F78&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8922</td>\n",
       "      <td>0.8934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function clean_all_digit at 0x0000023505146828&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8913</td>\n",
       "      <td>0.8994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function split_digit_letters at 0x0000023505146798&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 2), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.9048</td>\n",
       "      <td>0.9098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n   ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 3), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.9040</td>\n",
       "      <td>0.9094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n      ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(2, 2), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8876</td>\n",
       "      <td>0.8927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                         Model  \\\n",
       "0  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "1  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "2  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "3  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "4  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "5  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "6  LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n   ...   \n",
       "7  LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n      ...   \n",
       "\n",
       "                                          Preprocessing  \\\n",
       "0   <function none_preprocessing at 0x0000023505146168>   \n",
       "1           <function clean_html at 0x000002357CE3AAF8>   \n",
       "2    <function remove_duplicates at 0x0000023505460F78>   \n",
       "3      <function clean_all_digit at 0x0000023505146828>   \n",
       "4  <function split_digit_letters at 0x0000023505146798>   \n",
       "5   <function none_preprocessing at 0x0000023505146168>   \n",
       "6   <function none_preprocessing at 0x0000023505146168>   \n",
       "7   <function none_preprocessing at 0x0000023505146168>   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                    Vectorizer  \\\n",
       "0  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "1  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "2  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "3  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "4  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "5  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 2), preprocessor=None, stop_words=None...   \n",
       "6  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 3), preprocessor=None, stop_words=None...   \n",
       "7  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(2, 2), preprocessor=None, stop_words=None...   \n",
       "\n",
       "    f1 cv  f1 test  \n",
       "0  0.8909   0.8994  \n",
       "1  0.8909   0.8993  \n",
       "2  0.8922   0.8934  \n",
       "3  0.8913   0.8994  \n",
       "4  0.8909   0.8998  \n",
       "5  0.9048   0.9098  \n",
       "6  0.9040   0.9094  \n",
       "7  0.8876   0.8927  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_data_df(models_research_df, result_ngram_range_1_2)\n",
    "add_data_df(models_research_df, result_ngram_range_1_3)\n",
    "add_data_df(models_research_df, result_ngram_range_2_2)\n",
    "models_research_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The second step for vectorization is using the list Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=4000,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " <function __main__.none_preprocessing(data)>,\n",
       " CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, vocabulary=None),\n",
       " 0.886416323816726,\n",
       " 0.8919188560026605)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_stop_words = model_exploring(data_set=(data.review, data.sentiment), \n",
    "                                  preprocessing_fn=none_preprocessing, \n",
    "                                  vectorizer=CountVectorizer(stop_words='english'), \n",
    "                                  model_fn=LogisticRegression, \n",
    "                                  parameters=hyperparams_C)\n",
    "result_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Preprocessing</th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>f1 cv</th>\n",
       "      <th>f1 test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function clean_html at 0x000002357CE3AAF8&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function remove_duplicates at 0x0000023505460F78&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8922</td>\n",
       "      <td>0.8934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function clean_all_digit at 0x0000023505146828&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8913</td>\n",
       "      <td>0.8994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function split_digit_letters at 0x0000023505146798&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 2), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.9048</td>\n",
       "      <td>0.9098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n   ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 3), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.9040</td>\n",
       "      <td>0.9094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n      ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(2, 2), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8876</td>\n",
       "      <td>0.8927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words='eng...</td>\n",
       "      <td>0.8864</td>\n",
       "      <td>0.8919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                         Model  \\\n",
       "0  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "1  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "2  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "3  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "4  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "5  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "6  LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n   ...   \n",
       "7  LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n      ...   \n",
       "8  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "\n",
       "                                          Preprocessing  \\\n",
       "0   <function none_preprocessing at 0x0000023505146168>   \n",
       "1           <function clean_html at 0x000002357CE3AAF8>   \n",
       "2    <function remove_duplicates at 0x0000023505460F78>   \n",
       "3      <function clean_all_digit at 0x0000023505146828>   \n",
       "4  <function split_digit_letters at 0x0000023505146798>   \n",
       "5   <function none_preprocessing at 0x0000023505146168>   \n",
       "6   <function none_preprocessing at 0x0000023505146168>   \n",
       "7   <function none_preprocessing at 0x0000023505146168>   \n",
       "8   <function none_preprocessing at 0x0000023505146168>   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                    Vectorizer  \\\n",
       "0  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "1  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "2  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "3  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "4  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "5  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 2), preprocessor=None, stop_words=None...   \n",
       "6  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 3), preprocessor=None, stop_words=None...   \n",
       "7  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(2, 2), preprocessor=None, stop_words=None...   \n",
       "8  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words='eng...   \n",
       "\n",
       "    f1 cv  f1 test  \n",
       "0  0.8909   0.8994  \n",
       "1  0.8909   0.8993  \n",
       "2  0.8922   0.8934  \n",
       "3  0.8913   0.8994  \n",
       "4  0.8909   0.8998  \n",
       "5  0.9048   0.9098  \n",
       "6  0.9040   0.9094  \n",
       "7  0.8876   0.8927  \n",
       "8  0.8864   0.8919  "
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "add_data_df(models_research_df, result_stop_words)\n",
    "models_research_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The fourth step is min_df and max_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=4000,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " <function __main__.none_preprocessing(data)>,\n",
       " CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=200,\n",
       "                 ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, vocabulary=None),\n",
       " 0.8779439402218087,\n",
       " 0.8864439554577581)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_min_df = model_exploring(data_set=(data.review, data.sentiment), \n",
    "                                  preprocessing_fn=none_preprocessing, \n",
    "                                  vectorizer=CountVectorizer(min_df=200), \n",
    "                                  model_fn=LogisticRegression, \n",
    "                                  parameters=hyperparams_C)\n",
    "result_min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=4000,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " <function __main__.none_preprocessing(data)>,\n",
       " CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=0.99, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, vocabulary=None),\n",
       " 0.8908418846452433,\n",
       " 0.8985642737896494)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_max_df = model_exploring(data_set=(data.review, data.sentiment), \n",
    "                                  preprocessing_fn=none_preprocessing, \n",
    "                                  vectorizer=CountVectorizer(max_df=0.99), \n",
    "                                  model_fn=LogisticRegression, \n",
    "                                  parameters=hyperparams_C)\n",
    "result_max_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Preprocessing</th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>f1 cv</th>\n",
       "      <th>f1 test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function clean_html at 0x000002357CE3AAF8&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function remove_duplicates at 0x0000023505460F78&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8922</td>\n",
       "      <td>0.8934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function clean_all_digit at 0x0000023505146828&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8913</td>\n",
       "      <td>0.8994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function split_digit_letters at 0x0000023505146798&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 2), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.9048</td>\n",
       "      <td>0.9098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n   ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 3), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.9040</td>\n",
       "      <td>0.9094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n      ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(2, 2), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8876</td>\n",
       "      <td>0.8927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words='eng...</td>\n",
       "      <td>0.8864</td>\n",
       "      <td>0.8919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=200,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=No...</td>\n",
       "      <td>0.8779</td>\n",
       "      <td>0.8864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=0.99, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=Non...</td>\n",
       "      <td>0.8908</td>\n",
       "      <td>0.8986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                          Model  \\\n",
       "0   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "1   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "2   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "3   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "4   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "5   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "6   LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n   ...   \n",
       "7   LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n      ...   \n",
       "8   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "9   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "10  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "\n",
       "                                           Preprocessing  \\\n",
       "0    <function none_preprocessing at 0x0000023505146168>   \n",
       "1            <function clean_html at 0x000002357CE3AAF8>   \n",
       "2     <function remove_duplicates at 0x0000023505460F78>   \n",
       "3       <function clean_all_digit at 0x0000023505146828>   \n",
       "4   <function split_digit_letters at 0x0000023505146798>   \n",
       "5    <function none_preprocessing at 0x0000023505146168>   \n",
       "6    <function none_preprocessing at 0x0000023505146168>   \n",
       "7    <function none_preprocessing at 0x0000023505146168>   \n",
       "8    <function none_preprocessing at 0x0000023505146168>   \n",
       "9    <function none_preprocessing at 0x0000023505146168>   \n",
       "10   <function none_preprocessing at 0x0000023505146168>   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     Vectorizer  \\\n",
       "0   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "1   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "2   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "3   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "4   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "5   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 2), preprocessor=None, stop_words=None...   \n",
       "6   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 3), preprocessor=None, stop_words=None...   \n",
       "7   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(2, 2), preprocessor=None, stop_words=None...   \n",
       "8   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words='eng...   \n",
       "9   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=200,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=No...   \n",
       "10  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=0.99, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=Non...   \n",
       "\n",
       "     f1 cv  f1 test  \n",
       "0   0.8909   0.8994  \n",
       "1   0.8909   0.8993  \n",
       "2   0.8922   0.8934  \n",
       "3   0.8913   0.8994  \n",
       "4   0.8909   0.8998  \n",
       "5   0.9048   0.9098  \n",
       "6   0.9040   0.9094  \n",
       "7   0.8876   0.8927  \n",
       "8   0.8864   0.8919  \n",
       "9   0.8779   0.8864  \n",
       "10  0.8908   0.8986  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_data_df(models_research_df, result_min_df)\n",
    "add_data_df(models_research_df, result_max_df)\n",
    "models_research_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfIdfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply TfidfVectorizer with parameters:\n",
    "smooth_idfbool = False, in our set the situation when an extra document was seen containing every term in the collection exactly once is unlikely.\n",
    "sublinear_tf=True, replace tf with 1 + log(tf).(this parameter might be used for gridSearch)\n",
    "norm=None, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=4000,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " <function __main__.none_preprocessing(data)>,\n",
       " TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                 sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, use_idf=True, vocabulary=None),\n",
       " 0.8957970597601463,\n",
       " 0.9047714114593731)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_tfidf = model_exploring(data_set=(data.review, data.sentiment), \n",
    "                                  preprocessing_fn=none_preprocessing, \n",
    "                                  vectorizer=TfidfVectorizer(), \n",
    "                                  model_fn=LogisticRegression, \n",
    "                                  parameters=hyperparams_C)\n",
    "result_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Preprocessing</th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>f1 cv</th>\n",
       "      <th>f1 test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function clean_html at 0x000002357CE3AAF8&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function remove_duplicates at 0x0000023505460F78&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8922</td>\n",
       "      <td>0.8934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function clean_all_digit at 0x0000023505146828&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8913</td>\n",
       "      <td>0.8994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function split_digit_letters at 0x0000023505146798&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 2), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.9048</td>\n",
       "      <td>0.9098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n   ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 3), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.9040</td>\n",
       "      <td>0.9094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n      ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(2, 2), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8876</td>\n",
       "      <td>0.8927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words='eng...</td>\n",
       "      <td>0.8864</td>\n",
       "      <td>0.8919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=200,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=No...</td>\n",
       "      <td>0.8779</td>\n",
       "      <td>0.8864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=0.99, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=Non...</td>\n",
       "      <td>0.8908</td>\n",
       "      <td>0.8986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n     ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.float64'&gt;, encoding='utf-8',\\n                input='content', lowercase=True, max_df=1.0, max_features=None,\\n                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\\n ...</td>\n",
       "      <td>0.8958</td>\n",
       "      <td>0.9048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                          Model  \\\n",
       "0   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "1   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "2   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "3   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "4   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "5   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "6   LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n   ...   \n",
       "7   LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n      ...   \n",
       "8   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "9   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "10  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "11  LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n     ...   \n",
       "\n",
       "                                           Preprocessing  \\\n",
       "0    <function none_preprocessing at 0x0000023505146168>   \n",
       "1            <function clean_html at 0x000002357CE3AAF8>   \n",
       "2     <function remove_duplicates at 0x0000023505460F78>   \n",
       "3       <function clean_all_digit at 0x0000023505146828>   \n",
       "4   <function split_digit_letters at 0x0000023505146798>   \n",
       "5    <function none_preprocessing at 0x0000023505146168>   \n",
       "6    <function none_preprocessing at 0x0000023505146168>   \n",
       "7    <function none_preprocessing at 0x0000023505146168>   \n",
       "8    <function none_preprocessing at 0x0000023505146168>   \n",
       "9    <function none_preprocessing at 0x0000023505146168>   \n",
       "10   <function none_preprocessing at 0x0000023505146168>   \n",
       "11   <function none_preprocessing at 0x0000023505146168>   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     Vectorizer  \\\n",
       "0   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "1   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "2   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "3   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "4   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "5   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 2), preprocessor=None, stop_words=None...   \n",
       "6   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 3), preprocessor=None, stop_words=None...   \n",
       "7   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(2, 2), preprocessor=None, stop_words=None...   \n",
       "8   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words='eng...   \n",
       "9   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=200,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=No...   \n",
       "10  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=0.99, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=Non...   \n",
       "11  TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.float64'>, encoding='utf-8',\\n                input='content', lowercase=True, max_df=1.0, max_features=None,\\n                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\\n ...   \n",
       "\n",
       "     f1 cv  f1 test  \n",
       "0   0.8909   0.8994  \n",
       "1   0.8909   0.8993  \n",
       "2   0.8922   0.8934  \n",
       "3   0.8913   0.8994  \n",
       "4   0.8909   0.8998  \n",
       "5   0.9048   0.9098  \n",
       "6   0.9040   0.9094  \n",
       "7   0.8876   0.8927  \n",
       "8   0.8864   0.8919  \n",
       "9   0.8779   0.8864  \n",
       "10  0.8908   0.8986  \n",
       "11  0.8958   0.9048  "
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_data_df(models_research_df, result_tfidf)\n",
    "models_research_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=10000, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=4000,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " <function __main__.none_preprocessing(data)>,\n",
       " TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                 sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, use_idf=True, vocabulary=None),\n",
       " 0.9139108669635979,\n",
       " 0.9179084619478446)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_tfidf_ngramm_1_2 = model_exploring(data_set=(data.review, data.sentiment), \n",
    "                                  preprocessing_fn=none_preprocessing, \n",
    "                                  vectorizer=TfidfVectorizer(ngram_range=(1,2)), \n",
    "                                  model_fn=LogisticRegression, \n",
    "                                  parameters=hyperparams_C)\n",
    "result_tfidf_ngramm_1_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Preprocessing</th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>f1 cv</th>\n",
       "      <th>f1 test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function clean_html at 0x000002357CE3AAF8&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function remove_duplicates at 0x0000023505460F78&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8922</td>\n",
       "      <td>0.8934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function clean_all_digit at 0x0000023505146828&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8913</td>\n",
       "      <td>0.8994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function split_digit_letters at 0x0000023505146798&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 2), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.9048</td>\n",
       "      <td>0.9098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n   ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 3), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.9040</td>\n",
       "      <td>0.9094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n      ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(2, 2), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8876</td>\n",
       "      <td>0.8927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words='eng...</td>\n",
       "      <td>0.8864</td>\n",
       "      <td>0.8919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=200,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=No...</td>\n",
       "      <td>0.8779</td>\n",
       "      <td>0.8864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=0.99, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=Non...</td>\n",
       "      <td>0.8908</td>\n",
       "      <td>0.8986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n     ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.float64'&gt;, encoding='utf-8',\\n                input='content', lowercase=True, max_df=1.0, max_features=None,\\n                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\\n ...</td>\n",
       "      <td>0.8958</td>\n",
       "      <td>0.9048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(C=10000, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n  ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.float64'&gt;, encoding='utf-8',\\n                input='content', lowercase=True, max_df=1.0, max_features=None,\\n                min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\\n ...</td>\n",
       "      <td>0.9139</td>\n",
       "      <td>0.9179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                          Model  \\\n",
       "0   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "1   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "2   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "3   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "4   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "5   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "6   LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n   ...   \n",
       "7   LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n      ...   \n",
       "8   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "9   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "10  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "11  LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n     ...   \n",
       "12  LogisticRegression(C=10000, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n  ...   \n",
       "\n",
       "                                           Preprocessing  \\\n",
       "0    <function none_preprocessing at 0x0000023505146168>   \n",
       "1            <function clean_html at 0x000002357CE3AAF8>   \n",
       "2     <function remove_duplicates at 0x0000023505460F78>   \n",
       "3       <function clean_all_digit at 0x0000023505146828>   \n",
       "4   <function split_digit_letters at 0x0000023505146798>   \n",
       "5    <function none_preprocessing at 0x0000023505146168>   \n",
       "6    <function none_preprocessing at 0x0000023505146168>   \n",
       "7    <function none_preprocessing at 0x0000023505146168>   \n",
       "8    <function none_preprocessing at 0x0000023505146168>   \n",
       "9    <function none_preprocessing at 0x0000023505146168>   \n",
       "10   <function none_preprocessing at 0x0000023505146168>   \n",
       "11   <function none_preprocessing at 0x0000023505146168>   \n",
       "12   <function none_preprocessing at 0x0000023505146168>   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     Vectorizer  \\\n",
       "0   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "1   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "2   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "3   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "4   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "5   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 2), preprocessor=None, stop_words=None...   \n",
       "6   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 3), preprocessor=None, stop_words=None...   \n",
       "7   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(2, 2), preprocessor=None, stop_words=None...   \n",
       "8   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words='eng...   \n",
       "9   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=200,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=No...   \n",
       "10  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=0.99, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=Non...   \n",
       "11  TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.float64'>, encoding='utf-8',\\n                input='content', lowercase=True, max_df=1.0, max_features=None,\\n                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\\n ...   \n",
       "12  TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.float64'>, encoding='utf-8',\\n                input='content', lowercase=True, max_df=1.0, max_features=None,\\n                min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\\n ...   \n",
       "\n",
       "     f1 cv  f1 test  \n",
       "0   0.8909   0.8994  \n",
       "1   0.8909   0.8993  \n",
       "2   0.8922   0.8934  \n",
       "3   0.8913   0.8994  \n",
       "4   0.8909   0.8998  \n",
       "5   0.9048   0.9098  \n",
       "6   0.9040   0.9094  \n",
       "7   0.8876   0.8927  \n",
       "8   0.8864   0.8919  \n",
       "9   0.8779   0.8864  \n",
       "10  0.8908   0.8986  \n",
       "11  0.8958   0.9048  \n",
       "12  0.9139   0.9179  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_data_df(models_research_df, result_tfidf_ngramm_1_2)\n",
    "models_research_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply the preprocessing steps and the best vectorizer  for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data_: tuple):\n",
    "    return clean_html(data_=remove_duplicates(data_=split_digit_letters(data_=data_)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=10000, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=4000,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " <function __main__.preprocessing(data_: tuple)>,\n",
       " TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                 sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, use_idf=True, vocabulary=None),\n",
       " 0.9153568213187256,\n",
       " 0.9127052722558341)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_model = model_exploring(data_set=(data.review, data.sentiment), \n",
    "                                  preprocessing_fn=preprocessing, \n",
    "                                  vectorizer=TfidfVectorizer(ngram_range=(1,2)), \n",
    "                                  model_fn=LogisticRegression, \n",
    "                                  parameters=hyperparams_C)\n",
    "result_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Preprocessing</th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>f1 cv</th>\n",
       "      <th>f1 test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function clean_html at 0x000002357CE3AAF8&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function remove_duplicates at 0x0000023505460F78&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8922</td>\n",
       "      <td>0.8934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function clean_all_digit at 0x0000023505146828&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8913</td>\n",
       "      <td>0.8994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function split_digit_letters at 0x0000023505146798&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 2), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.9048</td>\n",
       "      <td>0.9098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n   ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 3), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.9040</td>\n",
       "      <td>0.9094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n      ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(2, 2), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8876</td>\n",
       "      <td>0.8927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words='eng...</td>\n",
       "      <td>0.8864</td>\n",
       "      <td>0.8919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=200,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=No...</td>\n",
       "      <td>0.8779</td>\n",
       "      <td>0.8864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=0.99, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=Non...</td>\n",
       "      <td>0.8908</td>\n",
       "      <td>0.8986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n     ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.float64'&gt;, encoding='utf-8',\\n                input='content', lowercase=True, max_df=1.0, max_features=None,\\n                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\\n ...</td>\n",
       "      <td>0.8958</td>\n",
       "      <td>0.9048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(C=10000, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n  ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.float64'&gt;, encoding='utf-8',\\n                input='content', lowercase=True, max_df=1.0, max_features=None,\\n                min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\\n ...</td>\n",
       "      <td>0.9139</td>\n",
       "      <td>0.9179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(C=10000, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n  ...</td>\n",
       "      <td>&lt;function preprocessing at 0x000002351115ADC8&gt;</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.float64'&gt;, encoding='utf-8',\\n                input='content', lowercase=True, max_df=1.0, max_features=None,\\n                min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\\n ...</td>\n",
       "      <td>0.9154</td>\n",
       "      <td>0.9127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                          Model  \\\n",
       "0   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "1   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "2   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "3   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "4   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "5   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "6   LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n   ...   \n",
       "7   LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n      ...   \n",
       "8   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "9   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "10  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "11  LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n     ...   \n",
       "12  LogisticRegression(C=10000, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n  ...   \n",
       "13  LogisticRegression(C=10000, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n  ...   \n",
       "\n",
       "                                           Preprocessing  \\\n",
       "0    <function none_preprocessing at 0x0000023505146168>   \n",
       "1            <function clean_html at 0x000002357CE3AAF8>   \n",
       "2     <function remove_duplicates at 0x0000023505460F78>   \n",
       "3       <function clean_all_digit at 0x0000023505146828>   \n",
       "4   <function split_digit_letters at 0x0000023505146798>   \n",
       "5    <function none_preprocessing at 0x0000023505146168>   \n",
       "6    <function none_preprocessing at 0x0000023505146168>   \n",
       "7    <function none_preprocessing at 0x0000023505146168>   \n",
       "8    <function none_preprocessing at 0x0000023505146168>   \n",
       "9    <function none_preprocessing at 0x0000023505146168>   \n",
       "10   <function none_preprocessing at 0x0000023505146168>   \n",
       "11   <function none_preprocessing at 0x0000023505146168>   \n",
       "12   <function none_preprocessing at 0x0000023505146168>   \n",
       "13        <function preprocessing at 0x000002351115ADC8>   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     Vectorizer  \\\n",
       "0   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "1   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "2   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "3   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "4   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "5   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 2), preprocessor=None, stop_words=None...   \n",
       "6   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 3), preprocessor=None, stop_words=None...   \n",
       "7   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(2, 2), preprocessor=None, stop_words=None...   \n",
       "8   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words='eng...   \n",
       "9   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=200,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=No...   \n",
       "10  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=0.99, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=Non...   \n",
       "11  TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.float64'>, encoding='utf-8',\\n                input='content', lowercase=True, max_df=1.0, max_features=None,\\n                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\\n ...   \n",
       "12  TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.float64'>, encoding='utf-8',\\n                input='content', lowercase=True, max_df=1.0, max_features=None,\\n                min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\\n ...   \n",
       "13  TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.float64'>, encoding='utf-8',\\n                input='content', lowercase=True, max_df=1.0, max_features=None,\\n                min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\\n ...   \n",
       "\n",
       "     f1 cv  f1 test  \n",
       "0   0.8909   0.8994  \n",
       "1   0.8909   0.8993  \n",
       "2   0.8922   0.8934  \n",
       "3   0.8913   0.8994  \n",
       "4   0.8909   0.8998  \n",
       "5   0.9048   0.9098  \n",
       "6   0.9040   0.9094  \n",
       "7   0.8876   0.8927  \n",
       "8   0.8864   0.8919  \n",
       "9   0.8779   0.8864  \n",
       "10  0.8908   0.8986  \n",
       "11  0.8958   0.9048  \n",
       "12  0.9139   0.9179  \n",
       "13  0.9154   0.9127  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_data_df(models_research_df, result_model)\n",
    "models_research_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_1(data_: tuple):\n",
    "    return clean_html(data_=remove_duplicates(data_=clean_all_digit(data_=data_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=10000, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=4000,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " <function __main__.preprocessing_1(data_: tuple)>,\n",
       " TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                 sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, use_idf=True, vocabulary=None),\n",
       " 0.9150690411772752,\n",
       " 0.9116297626171953)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_model_1 = model_exploring(data_set=(data.review, data.sentiment), \n",
    "                                  preprocessing_fn=preprocessing_1, \n",
    "                                  vectorizer=TfidfVectorizer(ngram_range=(1,2)), \n",
    "                                  model_fn=LogisticRegression, \n",
    "                                  parameters=hyperparams_C)\n",
    "result_model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Preprocessing</th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>f1 cv</th>\n",
       "      <th>f1 test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function clean_html at 0x000002357CE3AAF8&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function remove_duplicates at 0x0000023505460F78&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8922</td>\n",
       "      <td>0.8934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function clean_all_digit at 0x0000023505146828&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8913</td>\n",
       "      <td>0.8994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function split_digit_letters at 0x0000023505146798&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.8998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 2), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.9048</td>\n",
       "      <td>0.9098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n   ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 3), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.9040</td>\n",
       "      <td>0.9094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n      ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(2, 2), preprocessor=None, stop_words=None...</td>\n",
       "      <td>0.8876</td>\n",
       "      <td>0.8927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words='eng...</td>\n",
       "      <td>0.8864</td>\n",
       "      <td>0.8919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=200,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=No...</td>\n",
       "      <td>0.8779</td>\n",
       "      <td>0.8864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',\\n                lowercase=True, max_df=0.99, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=Non...</td>\n",
       "      <td>0.8908</td>\n",
       "      <td>0.8986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n     ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.float64'&gt;, encoding='utf-8',\\n                input='content', lowercase=True, max_df=1.0, max_features=None,\\n                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\\n ...</td>\n",
       "      <td>0.8958</td>\n",
       "      <td>0.9048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(C=10000, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n  ...</td>\n",
       "      <td>&lt;function none_preprocessing at 0x0000023505146168&gt;</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.float64'&gt;, encoding='utf-8',\\n                input='content', lowercase=True, max_df=1.0, max_features=None,\\n                min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\\n ...</td>\n",
       "      <td>0.9139</td>\n",
       "      <td>0.9179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(C=10000, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n  ...</td>\n",
       "      <td>&lt;function preprocessing at 0x000002351115ADC8&gt;</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.float64'&gt;, encoding='utf-8',\\n                input='content', lowercase=True, max_df=1.0, max_features=None,\\n                min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\\n ...</td>\n",
       "      <td>0.9154</td>\n",
       "      <td>0.9127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(C=10000, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n  ...</td>\n",
       "      <td>&lt;function preprocessing_1 at 0x000002351B9494C8&gt;</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=&lt;class 'numpy.float64'&gt;, encoding='utf-8',\\n                input='content', lowercase=True, max_df=1.0, max_features=None,\\n                min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\\n ...</td>\n",
       "      <td>0.9151</td>\n",
       "      <td>0.9116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                          Model  \\\n",
       "0   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "1   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "2   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "3   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "4   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "5   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "6   LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n   ...   \n",
       "7   LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n      ...   \n",
       "8   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "9   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "10  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n    ...   \n",
       "11  LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n     ...   \n",
       "12  LogisticRegression(C=10000, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n  ...   \n",
       "13  LogisticRegression(C=10000, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n  ...   \n",
       "14  LogisticRegression(C=10000, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\\n                   multi_class='auto', n_jobs=None, penalty='l2',\\n                   random_state=11, solver='lbfgs', tol=0.0001, verbose=0,\\n  ...   \n",
       "\n",
       "                                           Preprocessing  \\\n",
       "0    <function none_preprocessing at 0x0000023505146168>   \n",
       "1            <function clean_html at 0x000002357CE3AAF8>   \n",
       "2     <function remove_duplicates at 0x0000023505460F78>   \n",
       "3       <function clean_all_digit at 0x0000023505146828>   \n",
       "4   <function split_digit_letters at 0x0000023505146798>   \n",
       "5    <function none_preprocessing at 0x0000023505146168>   \n",
       "6    <function none_preprocessing at 0x0000023505146168>   \n",
       "7    <function none_preprocessing at 0x0000023505146168>   \n",
       "8    <function none_preprocessing at 0x0000023505146168>   \n",
       "9    <function none_preprocessing at 0x0000023505146168>   \n",
       "10   <function none_preprocessing at 0x0000023505146168>   \n",
       "11   <function none_preprocessing at 0x0000023505146168>   \n",
       "12   <function none_preprocessing at 0x0000023505146168>   \n",
       "13        <function preprocessing at 0x000002351115ADC8>   \n",
       "14      <function preprocessing_1 at 0x000002351B9494C8>   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     Vectorizer  \\\n",
       "0   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "1   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "2   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "3   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "4   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=None...   \n",
       "5   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 2), preprocessor=None, stop_words=None...   \n",
       "6   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 3), preprocessor=None, stop_words=None...   \n",
       "7   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(2, 2), preprocessor=None, stop_words=None...   \n",
       "8   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words='eng...   \n",
       "9   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=1.0, max_features=None, min_df=200,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=No...   \n",
       "10  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\\n                lowercase=True, max_df=0.99, max_features=None, min_df=1,\\n                ngram_range=(1, 1), preprocessor=None, stop_words=Non...   \n",
       "11  TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.float64'>, encoding='utf-8',\\n                input='content', lowercase=True, max_df=1.0, max_features=None,\\n                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\\n ...   \n",
       "12  TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.float64'>, encoding='utf-8',\\n                input='content', lowercase=True, max_df=1.0, max_features=None,\\n                min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\\n ...   \n",
       "13  TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.float64'>, encoding='utf-8',\\n                input='content', lowercase=True, max_df=1.0, max_features=None,\\n                min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\\n ...   \n",
       "14  TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\\n                dtype=<class 'numpy.float64'>, encoding='utf-8',\\n                input='content', lowercase=True, max_df=1.0, max_features=None,\\n                min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\\n ...   \n",
       "\n",
       "     f1 cv  f1 test  \n",
       "0   0.8909   0.8994  \n",
       "1   0.8909   0.8993  \n",
       "2   0.8922   0.8934  \n",
       "3   0.8913   0.8994  \n",
       "4   0.8909   0.8998  \n",
       "5   0.9048   0.9098  \n",
       "6   0.9040   0.9094  \n",
       "7   0.8876   0.8927  \n",
       "8   0.8864   0.8919  \n",
       "9   0.8779   0.8864  \n",
       "10  0.8908   0.8986  \n",
       "11  0.8958   0.9048  \n",
       "12  0.9139   0.9179  \n",
       "13  0.9154   0.9127  \n",
       "14  0.9151   0.9116  "
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_data_df(models_research_df, result_model_1)\n",
    "models_research_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_f1_curve(title='',\n",
    "             lines=(models_research_df['f1 cv'], models_research_df['f1 test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
